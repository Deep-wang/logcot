import re
import os
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
import pandas as pd
from collections import defaultdict

class LogTimeExtractor:
    """æ—¥å¿—æ—¶é—´æå–å™¨ï¼Œæ”¯æŒå¤šç§æ—¶é—´æ ¼å¼"""
    
    def __init__(self):
        # å®šä¹‰ä¸åŒçš„æ—¶é—´æ ¼å¼æ­£åˆ™è¡¨è¾¾å¼
        self.time_patterns = {
            # æ ¼å¼1: Apr 23 14:56:46 (æœˆ æ—¥ æ—¶:åˆ†:ç§’)
            'syslog_format': r'(\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2})',
            
            # æ ¼å¼2: 2025-04-23 14:42:48.038 (å¹´-æœˆ-æ—¥ æ—¶:åˆ†:ç§’.æ¯«ç§’)
            'standard_format': r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}(?:\.\d{3})?)',
            
            # æ ¼å¼3: 2025-04-23T11:54:00.805444+08:00 (ISO 8601æ ¼å¼)
            'iso_format': r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d{6})?[+-]\d{2}:\d{2})',
            
            # æ ¼å¼4: ä»…æ—¥æœŸæ—¶é—´ï¼Œç”¨äºè¡¨æ ¼æ ¼å¼ 2025-04-23 14:39:24
            'simple_datetime': r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'
        }
        
        # æœˆä»½æ˜ å°„
        self.month_map = {
            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
        }
    
    def extract_time_from_line(self, log_line: str, current_year: int = 2025) -> Optional[datetime]:
        """
        ä»å•è¡Œæ—¥å¿—ä¸­æå–æ—¶é—´ä¿¡æ¯
        
        Args:
            log_line: æ—¥å¿—è¡Œå†…å®¹
            current_year: å½“å‰å¹´ä»½ï¼Œç”¨äºsyslogæ ¼å¼çš„å¹´ä»½è¡¥å……
            
        Returns:
            datetimeå¯¹è±¡æˆ–None
        """
        log_line = log_line.strip()
        
        # å°è¯•åŒ¹é…å„ç§æ ¼å¼
        for format_name, pattern in self.time_patterns.items():
            match = re.search(pattern, log_line)
            if match:
                time_str = match.group(1)
                try:
                    parsed_time = self._parse_time_string(time_str, format_name, current_year)
                    if parsed_time:
                        return parsed_time
                except Exception as e:
                    continue
        
        return None
    
    def _parse_time_string(self, time_str: str, format_name: str, current_year: int) -> Optional[datetime]:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºdatetimeå¯¹è±¡"""
        
        try:
            if format_name == 'syslog_format':
                # å¤„ç† Apr 23 14:56:46 æ ¼å¼
                parts = time_str.split()
                month_str = parts[0]
                day = int(parts[1])
                time_part = parts[2]
                hour, minute, second = map(int, time_part.split(':'))
                
                month = int(self.month_map[month_str])
                return datetime(current_year, month, day, hour, minute, second)
                
            elif format_name == 'standard_format':
                # å¤„ç† 2025-04-23 14:42:48.038 æ ¼å¼
                if '.' in time_str:
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S.%f')
                else:
                    return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                    
            elif format_name == 'iso_format':
                # å¤„ç† 2025-04-23T11:54:00.805444+08:00 æ ¼å¼
                # ç§»é™¤æ—¶åŒºä¿¡æ¯è¿›è¡Œç®€å•è§£æ
                # å…ˆåˆ†ç¦»æ—¶åŒºéƒ¨åˆ†
                if '+' in time_str:
                    base_time = time_str.split('+')[0]
                elif time_str.count('-') > 2:  # æœ‰æ—¶åŒºçš„è´Ÿå·
                    # æ‰¾åˆ°æœ€åä¸€ä¸ª'-'ï¼Œå®ƒåº”è¯¥æ˜¯æ—¶åŒºåˆ†éš”ç¬¦
                    parts = time_str.split('-')
                    if len(parts) >= 4:  # è‡³å°‘æœ‰å¹´-æœˆ-æ—¥-æ—¶åŒº
                        base_time = '-'.join(parts[:-1])
                    else:
                        base_time = time_str
                else:
                    base_time = time_str
                
                # å°†Tæ›¿æ¢ä¸ºç©ºæ ¼
                if 'T' in base_time:
                    base_time = base_time.replace('T', ' ')
                
                # å¤„ç†å¾®ç§’ç²¾åº¦
                if '.' in base_time:
                    # ç¡®ä¿å¾®ç§’éƒ¨åˆ†ä¸è¶…è¿‡6ä½
                    dot_pos = base_time.find('.')
                    base_part = base_time[:dot_pos]
                    microsec_part = base_time[dot_pos+1:]
                    # æˆªå–æˆ–å¡«å……åˆ°6ä½
                    if len(microsec_part) > 6:
                        microsec_part = microsec_part[:6]
                    else:
                        microsec_part = microsec_part.ljust(6, '0')
                    base_time = base_part + '.' + microsec_part
                    return datetime.strptime(base_time, '%Y-%m-%d %H:%M:%S.%f')
                else:
                    return datetime.strptime(base_time, '%Y-%m-%d %H:%M:%S')
                    
            elif format_name == 'simple_datetime':
                # å¤„ç† 2025-04-23 14:39:24 æ ¼å¼
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                
        except Exception as e:
            pass
            
        return None

class LogTimeGrouper:
    """æŒ‰æ—¶é—´åˆ†ç»„æ—¥å¿—å¤„ç†å™¨"""
    
    def __init__(self, group_hours: int = 2):
        """
        åˆå§‹åŒ–åˆ†ç»„å™¨
        
        Args:
            group_hours: åˆ†ç»„æ—¶é—´é—´éš”ï¼ˆå°æ—¶ï¼‰
        """
        self.group_hours = group_hours
        self.extractor = LogTimeExtractor()
    
    def process_log_file(self, log_file_path: str, output_dir: str = None, base_dir: str = None) -> Dict[str, List[str]]:
        """
        å¤„ç†å•ä¸ªæ—¥å¿—æ–‡ä»¶ï¼ŒæŒ‰æ—¶é—´åˆ†ç»„
        
        Args:
            log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœæŒ‡å®šåˆ™ä¿å­˜åˆ†ç»„æ–‡ä»¶
            base_dir: åŸºç¡€ç›®å½•ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            
        Returns:
            æŒ‰æ—¶é—´åˆ†ç»„çš„æ—¥å¿—å­—å…¸
        """
        if not os.path.exists(log_file_path):
            raise FileNotFoundError(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
            
        # è¯»å–æ—¥å¿—æ–‡ä»¶
        with open(log_file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        # æŒ‰æ—¶é—´åˆ†ç»„
        grouped_logs = self._group_logs_by_time(lines)
        
        # å¦‚æœæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä¿å­˜åˆ†ç»„æ–‡ä»¶
        if output_dir:
            self._save_grouped_logs(grouped_logs, output_dir, log_file_path, base_dir)
            
        return grouped_logs
    
    def process_log_directory(self, input_dir: str, output_dir: str = None) -> Dict[str, Dict[str, List[str]]]:
        """
        å¤„ç†æ•´ä¸ªç›®å½•çš„æ—¥å¿—æ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æŒ‰æ–‡ä»¶å’Œæ—¶é—´åˆ†ç»„çš„æ—¥å¿—å­—å…¸
        """
        if not os.path.exists(input_dir):
            raise FileNotFoundError(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        all_grouped_logs = {}
        
        # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for filename in os.listdir(input_dir):
            file_path = os.path.join(input_dir, filename)
            
            # è·³è¿‡ç›®å½•å’Œéæ—¥å¿—æ–‡ä»¶
            if os.path.isdir(file_path) or not self._is_log_file(filename):
                continue
                
            print(f"æ­£åœ¨å¤„ç†æ—¥å¿—æ–‡ä»¶: {filename}")
            
            try:
                grouped_logs = self.process_log_file(file_path, output_dir, input_dir)
                all_grouped_logs[filename] = grouped_logs
                print(f"âœ… {filename} å¤„ç†å®Œæˆï¼Œå…±åˆ†ä¸º {len(grouped_logs)} ä¸ªæ—¶é—´ç»„")
            except Exception as e:
                print(f"âŒ å¤„ç† {filename} æ—¶å‡ºé”™: {e}")
                
        return all_grouped_logs
    
    def _group_logs_by_time(self, lines: List[str]) -> Dict[str, List[str]]:
        """æŒ‰æ—¶é—´åˆ†ç»„æ—¥å¿—è¡Œ"""
        grouped_logs = defaultdict(list)
        current_group_key = None  # è®°å½•å½“å‰çš„æ—¶é—´ç»„
        
        for line in lines:
            if not line.strip():
                continue
                
            # æå–æ—¶é—´
            log_time = self.extractor.extract_time_from_line(line)
            
            if log_time:
                # èƒ½å¤Ÿè§£ææ—¶é—´ï¼Œè®¡ç®—æ—¶é—´ç»„æ ‡è¯†
                group_key = self._get_time_group_key(log_time)
                grouped_logs[group_key].append(line.strip())
                current_group_key = group_key  # æ›´æ–°å½“å‰æ—¶é—´ç»„
            else:
                # æ— æ³•æå–æ—¶é—´çš„æ—¥å¿—ï¼Œæ”¾å…¥å½“å‰æ—¶é—´ç»„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if current_group_key:
                    grouped_logs[current_group_key].append(line.strip())
                # å¦‚æœè¿˜æ²¡æœ‰ä»»ä½•æ—¶é—´ç»„ï¼Œæš‚æ—¶è·³è¿‡è¿™è¡Œï¼ˆé€šå¸¸æ˜¯æ–‡ä»¶å¼€å¤´çš„ä¸€äº›æ— å…³ä¿¡æ¯ï¼‰
                
        return dict(grouped_logs)
    
    def _get_time_group_key(self, log_time: datetime) -> str:
        """è·å–æ—¶é—´åˆ†ç»„é”®"""
        # è®¡ç®—è¯¥æ—¶é—´å±äºå“ªä¸ªæ—¶é—´ç»„
        base_hour = (log_time.hour // self.group_hours) * self.group_hours
        group_start = log_time.replace(hour=base_hour, minute=0, second=0, microsecond=0)
        group_end = group_start + timedelta(hours=self.group_hours)
        
        return f"{group_start.strftime('%Y-%m-%d_%H%M')}-{group_end.strftime('%H%M')}"
    
    def _save_grouped_logs(self, grouped_logs: Dict[str, List[str]], output_dir: str, log_file_path: str, base_dir: str):
        """ä¿å­˜åˆ†ç»„åçš„æ—¥å¿—"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åå‰ç¼€
        if base_dir:
            # è®¡ç®—ç›¸å¯¹äºbase_dirçš„ç›¸å¯¹è·¯å¾„
            try:
                rel_path = os.path.relpath(log_file_path, base_dir)
                # å°†è·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œç”Ÿæˆå”¯ä¸€å‰ç¼€
                unique_prefix = rel_path.replace(os.sep, '_').replace('/', '_').replace('\\', '_')
                # ç§»é™¤æ–‡ä»¶æ‰©å±•å
                unique_prefix = os.path.splitext(unique_prefix)[0]
            except ValueError:
                # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨æ–‡ä»¶å
                unique_prefix = os.path.splitext(os.path.basename(log_file_path))[0]
        else:
            unique_prefix = os.path.splitext(os.path.basename(log_file_path))[0]
        
        for group_key, logs in grouped_logs.items():
            if not logs:
                continue
                
            output_filename = f"{unique_prefix}_{group_key}.log"
            output_path = os.path.join(output_dir, output_filename)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                for log in logs:
                    f.write(log + '\n')
                    
            print(f"  ğŸ“ ä¿å­˜æ—¶é—´ç»„ {group_key}: {len(logs)} æ¡æ—¥å¿— -> {output_filename}")
    
    def _is_log_file(self, filename: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ—¥å¿—æ–‡ä»¶"""
        log_extensions = ['.log', '.txt', '.out', '']
        return any(filename.lower().endswith(ext) for ext in log_extensions)
    
    def generate_time_analysis_report(self, grouped_logs: Dict[str, List[str]]) -> str:
        """ç”Ÿæˆæ—¶é—´åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("æ—¥å¿—æ—¶é—´åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        total_logs = sum(len(logs) for logs in grouped_logs.values())
        report.append(f"æ€»æ—¥å¿—æ¡æ•°: {total_logs}")
        report.append(f"æ—¶é—´åˆ†ç»„æ•°: {len(grouped_logs)}")
        report.append(f"åˆ†ç»„é—´éš”: {self.group_hours} å°æ—¶")
        report.append("-" * 60)
        
        # æŒ‰æ—¶é—´ç»„æ’åº
        sorted_groups = sorted(grouped_logs.items(), 
                             key=lambda x: x[0] if x[0] != 'unknown_time' else 'zzz')
        
        for group_key, logs in sorted_groups:
            report.append(f"æ—¶é—´ç»„: {group_key}")
            report.append(f"  æ—¥å¿—æ¡æ•°: {len(logs)}")
            if logs:
                # æ˜¾ç¤ºè¯¥æ—¶é—´ç»„çš„å‰å‡ æ¡æ—¥å¿—ä½œä¸ºç¤ºä¾‹
                report.append("  ç¤ºä¾‹æ—¥å¿—:")
                for i, log in enumerate(logs[:3]):
                    report.append(f"    {i+1}. {log[:100]}..." if len(log) > 100 else f"    {i+1}. {log}")
            report.append("-" * 40)
            
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # åˆ›å»ºæ—¶é—´åˆ†ç»„å™¨ï¼ˆ2å°æ—¶ä¸ºä¸€ç»„ï¼‰
    grouper = LogTimeGrouper(group_hours=2)
    
    # å¤„ç†ç¤ºä¾‹ï¼šå¤„ç†å•ä¸ªæ–‡ä»¶
    # grouped_logs = grouper.process_log_file('sample.log', 'output/')
    
    # å¤„ç†ç¤ºä¾‹ï¼šå¤„ç†æ•´ä¸ªç›®å½•
    # all_grouped = grouper.process_log_directory('./log/log/', './log/preprocessed/')
    
    # ç”ŸæˆæŠ¥å‘Šç¤ºä¾‹
    # report = grouper.generate_time_analysis_report(grouped_logs)
    # print(report)
    
    print("æ—¥å¿—é¢„å¤„ç†æ¨¡å—å·²åŠ è½½ã€‚ä½¿ç”¨æ–¹æ³•:")
    print("1. grouper = LogTimeGrouper(group_hours=2)")
    print("2. grouped_logs = grouper.process_log_file('your_log.log', 'output_dir/')")
    print("3. report = grouper.generate_time_analysis_report(grouped_logs)")

if __name__ == "__main__":
    main() 