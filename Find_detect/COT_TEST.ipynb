{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1a6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COT 方法，最新方法\n",
    "\"\"\"\n",
    "\n",
    "from math import log\n",
    "from tkinter import W\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import re\n",
    "import textwrap\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# 基于Prompt的分析\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "import scan.summerize as analyze_log_final\n",
    "\n",
    "\n",
    "@retry(stop=stop_after_attempt(7),\n",
    "       wait=wait_exponential(multiplier=2, min=2, max=100))\n",
    "def post_with_retry(payload, headers, API_URL):\n",
    "    response = requests.post(API_URL, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def generate_prompt(prompt_header,\n",
    "                    logs: List[str],\n",
    "                    max_len=1000,\n",
    "                    no_reason=False) -> tuple:\n",
    "    \"\"\"\n",
    "    生成prompt并保存对应的原始log及编号\n",
    "    :return: (prompt_parts, prompt_parts_count, log_parts)\n",
    "    \"\"\"\n",
    "    prompt_parts_count = []\n",
    "    prompt_parts = []\n",
    "    log_parts = []  # 保存每个prompt对应的原始log列表\n",
    "    prompt = prompt_header\n",
    "    log_count = 0\n",
    "    current_logs = []  # 当前prompt对应的log列表\n",
    "\n",
    "    for i, log in enumerate(logs):\n",
    "        log_str = f\"({i+1}) {log}\"  # 为每个log添加编号\n",
    "        log_length = len(log_str)\n",
    "        prompt_length = len(prompt)\n",
    "\n",
    "        if log_length > max_len:\n",
    "            print(\"warning: this log is too long\")\n",
    "\n",
    "        if prompt_length + log_length <= max_len:\n",
    "            prompt += f\" {log_str}\"\n",
    "            current_logs.append(f\"({i+1}) {log}\")  # 保存编号和原始log\n",
    "            prompt_length += log_length + 1\n",
    "            log_count += 1\n",
    "\n",
    "            if i < (len(logs) - 1) and (prompt_length +\n",
    "                                        len(logs[i + 1])) >= max_len:\n",
    "                prompt_parts.append(\n",
    "                    prompt.replace(\"!!NumberControl!!\", str(log_count)))\n",
    "                prompt_parts_count.append(log_count)\n",
    "                log_parts.append(current_logs)\n",
    "                log_count = 0\n",
    "                current_logs = []\n",
    "                prompt = prompt_header\n",
    "                continue\n",
    "\n",
    "            if i == (len(logs) - 1):\n",
    "                prompt_parts.append(\n",
    "                    prompt.replace(\"!!NumberControl!!\", str(log_count)))\n",
    "                prompt_parts_count.append(log_count)\n",
    "                log_parts.append(current_logs)\n",
    "        else:\n",
    "            if prompt != prompt_header:\n",
    "                log_count += 1\n",
    "                prompt += f\" {log_str}\"\n",
    "                current_logs.append(f\"({i+1}) {log}\")\n",
    "                prompt_parts.append(\n",
    "                    prompt.replace(\"!!NumberControl!!\", str(log_count)))\n",
    "                prompt_parts_count.append(log_count)\n",
    "                log_parts.append(current_logs)\n",
    "            else:\n",
    "                prompt = f\"{prompt} ({i+1}) {log}\"\n",
    "                current_logs.append(f\"({i+1}) {log}\")\n",
    "                prompt_parts.append(prompt)\n",
    "                prompt_parts_count.append(1)\n",
    "                log_parts.append(current_logs)\n",
    "\n",
    "            log_count = 0\n",
    "            current_logs = []\n",
    "            prompt = prompt_header\n",
    "\n",
    "    return prompt_parts, prompt_parts_count, log_parts\n",
    "\n",
    "\n",
    "def reprompt(raw_file_name, j, df_raw_answer, api_key, api_url, temperature):\n",
    "    prompt = df_raw_answer.loc[j, \"prompt\"]\n",
    "    msgs = []\n",
    "    payload = {\n",
    "        \"model\": \"Qwen/Qwen3-8B\",  #  \"THUDM/GLM-4-9B-0414\",\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 8192,\n",
    "        \"enable_thinking\": False,\n",
    "        \"thinking_budget\": 4096,\n",
    "        \"min_p\": 0.05,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 0.7,\n",
    "        \"top_k\": 50,\n",
    "        \"frequency_penalty\": 0.5,\n",
    "        \"n\": 1,\n",
    "        \"stop\": [],\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    parsed_log = \"\"\n",
    "    try:\n",
    "        text = post_with_retry(payload, headers, api_url)\n",
    "        parsed_log = text['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"error! 处理 prompt 失败 (密钥: {api_key[:10]}...): {e}\")\n",
    "        # return f\"分析失败: {e}\"\n",
    "\n",
    "    df_raw_answer.loc[j, \"answer\"] = parsed_log\n",
    "    df_raw_answer.to_excel(raw_file_name, index=False)\n",
    "    return parsed_log\n",
    "\n",
    "\n",
    "def extract_log_index(prompts):\n",
    "    log_numbers = []\n",
    "    for prompt in prompts:\n",
    "        log_number = re.findall(\n",
    "            r'\\((\\d{1,4})',\n",
    "            prompt.split(\"Organize your answer to be the following format\")\n",
    "            [1].split('a binary choice between')[0])\n",
    "        # log_number=re.findall(r'\\((\\d+)\\)',prompt.split(\"Organize your answer to be the following format\")[1].split('a binary choice between')[0])\n",
    "        log_numbers.append(sorted(list(set([int(x) for x in log_number]))))\n",
    "    return log_numbers\n",
    "\n",
    "\n",
    "def filter_numbers(text):\n",
    "    pattern = r'\\(\\d+\\)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def write_to_excel(raw_file_name, df_raw_answer, logs):\n",
    "    answer_list = df_raw_answer.iloc[:, 2].tolist()\n",
    "    logs.insert(0, \"log_content\")\n",
    "    # 匹配所有(x,y)格式的数据\n",
    "    pattern = r'\\((\\d+),\\s*([^)]*)\\)'  # 匹配(数字,内容)\n",
    "    matched_results = []\n",
    "    for text in answer_list:\n",
    "        matches = re.findall(pattern, str(text))\n",
    "        for match in matches:\n",
    "            matched_results.append(\n",
    "                (int(match[0]), match[1].strip()))  # 转换为(数字,内容)元组\n",
    "    pattern1 = r'\\((\\d+),([^)]*)\\)'  # (数字,内容)\n",
    "    # pattern2 = r'\\((\\d+),\\)'         # (数字,)\n",
    "    # pattern3 = r'\\((\\d+),'\n",
    "    # 使用集合来存储唯一结果\n",
    "    unique_results = []\n",
    "    for text in answer_list:\n",
    "        # 定义三种匹配模式\n",
    "        matches = re.findall(pattern1, str(text))\n",
    "        for match in matches:\n",
    "            unique_results.append(list(match))\n",
    "    for ls in unique_results:\n",
    "        ls[0] = int(ls[0])\n",
    "        if ls[1] == '0' or ls[1] == ' 0' or ls[1] == '0 ':\n",
    "            ls[1] = 'normal'\n",
    "        elif ls[1] == '1' or ls[1] == ' 1' or ls[1] == '1 ':\n",
    "            ls[1] = 'abnormal'\n",
    "    sorted_results = sorted(unique_results, key=lambda x: x[0])[:-3]\n",
    "    # print(sorted_results[:-3] )\n",
    "    for ls in sorted_results:\n",
    "        ls[0] = int(ls[0])\n",
    "    # for i in range(len(sorted_results)):\n",
    "    #     print(logs[i])\n",
    "    ANSWER_LIST = []\n",
    "    for rel in sorted_results:\n",
    "        try:\n",
    "            index = rel[0]\n",
    "            result = rel[1]\n",
    "            # print(index)\n",
    "            log_content = logs[index]\n",
    "            ANSWER_LIST.append([index, log_content, result])\n",
    "        except:\n",
    "            continue\n",
    "    # 将结果写入Excel文件\n",
    "    # 找出缺失的index\n",
    "    all_indices = set(range(1, len(logs)))  # 所有可能的index\n",
    "    found_indices = {x[0] for x in sorted_results}  # 已找到的index\n",
    "    missing_indices = sorted(all_indices - found_indices)  # 缺失的index\n",
    "\n",
    "    # 创建结果DataFrame\n",
    "    ANSWER_LIST = []\n",
    "    for rel in sorted_results:\n",
    "        try:  # todo: 这块后续要解决掉\n",
    "            index = rel[0]\n",
    "            result = rel[1]\n",
    "            log_content = logs[index] if index < len(logs) else \"\"\n",
    "            ANSWER_LIST.append([index, log_content, result])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # 添加缺失的index记录\n",
    "    for missing_idx in missing_indices:\n",
    "        log_content = logs[missing_idx] if missing_idx < len(logs) else \"\"\n",
    "        ANSWER_LIST.append([missing_idx, log_content, 'UNKNOWN'])\n",
    "\n",
    "    # 按index排序最终结果\n",
    "    ANSWER_LIST = sorted(ANSWER_LIST, key=lambda x: x[0])\n",
    "    OUT_raw_path = raw_file_name.replace('.xlsx', 'Aligned_final.xlsx')\n",
    "    df = pd.DataFrame(ANSWER_LIST, columns=['index', 'log_content', 'result'])\n",
    "    df.set_index('index', inplace=True)\n",
    "    df.to_excel(OUT_raw_path, index=False)\n",
    "    return OUT_raw_path\n",
    "\n",
    "\n",
    "def parse_logs(api_keys, api_url, prompt_parts: List[str], prompt_parts_count,\n",
    "               log_parts, raw_file_name) -> List[str]:\n",
    "    parsed_logs = []\n",
    "\n",
    "    # 定义单任务处理函数（接收 prompt 和对应的 api_key）\n",
    "    def process_prompt(prompt, api_key):\n",
    "        payload = {\n",
    "            \"model\": \"THUDM/GLM-4-9B-0414\",\n",
    "            \"stream\": False,\n",
    "            \"max_tokens\": 8192,\n",
    "            \"enable_thinking\": True,\n",
    "            \"thinking_budget\": 4096,\n",
    "            \"min_p\": 0.05,\n",
    "            # \"temperature\": 0.4,\n",
    "            # \"top_p\": 0.7,\n",
    "            # \"top_k\": 50,\n",
    "            # \"frequency_penalty\": 0.5,\n",
    "            \"temperature\": 0.1,  # 降低随机性，提高准确性\n",
    "            \"top_p\": 0.3,  # 更保守的选择\n",
    "            \"top_k\": 20,  # 减少候选词数量\n",
    "            \"frequency_penalty\": 0.2,  # 轻微减少重复\n",
    "            \"n\": 1,\n",
    "            \"stop\": [],\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            text = post_with_retry(payload, headers, api_url)\n",
    "            return text['choices'][0]['message']['content']\n",
    "        except Exception as e:\n",
    "            print(f\"error! 处理 prompt 失败 (密钥: {api_key[:10]}...): {e}\")\n",
    "            return f\"分析失败: {e}\"\n",
    "\n",
    "    # 多线程并发处理（结合 API 密钥轮询）\n",
    "    max_workers = min(5, len(api_keys))  # 根据密钥数量限制并发（避免单密钥超限制）\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 为每个 prompt 分配对应的 API 密钥（轮询）\n",
    "        futures = []\n",
    "        for idx, prompt in enumerate(prompt_parts):\n",
    "            selected_key = api_keys[idx % len(api_keys)]  # 轮询选择密钥\n",
    "            futures.append(\n",
    "                executor.submit(process_prompt, prompt, selected_key))\n",
    "\n",
    "        # 带进度条的结果收集\n",
    "        for future in tqdm(as_completed(futures),\n",
    "                           total=len(prompt_parts),\n",
    "                           desc=\"日志解析进度\"):\n",
    "            parsed_log = future.result()\n",
    "            parsed_logs.append(parsed_log)\n",
    "    pd.DataFrame(data=list(zip(prompt_parts, parsed_logs, log_parts)),\n",
    "                 columns=['prompt', 'answer', 'logs']).to_excel(raw_file_name)\n",
    "    return parsed_logs\n",
    "\n",
    "\n",
    "def read_error_logs(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    unkonwn_logs = df[df['result'] == 'UNKNOWN']['log_content'].tolist()\n",
    "    error_logs = df[df['result'] == 'abnormal']['log_content'].tolist()\n",
    "    bool_logs = (df['result'] == 'abnormal') | (df['result'] == 'UNKNOWN')\n",
    "    unknown_error_logs = df[bool_logs]['log_content'].tolist()\n",
    "    return unkonwn_logs, error_logs, unknown_error_logs\n",
    "\n",
    "\n",
    "def anylysis_error_logs(unkonwn_logs, error_logs, api_keys, api_url,\n",
    "                        analyze_out_dir):\n",
    "\n",
    "    def analyze_batch(batch, log_type, api_key):\n",
    "        prompt = f\"分析以下{log_type}日志的共同特征和可能原因:\\n\" + \"\\n\".join(batch)\n",
    "        payload = {\n",
    "            \"model\": \"THUDM/GLM-4-9B-0414\",\n",
    "            \"messages\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }]\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(api_url, json=payload, headers=headers)\n",
    "            return {\n",
    "                \"type\": log_type,\n",
    "                \"batch\": batch,\n",
    "                \"analysis\": response.json()['choices'][0]['message']['content']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"type\": log_type, \"batch\": batch, \"error\": f\"分析失败: {e}\"}\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(analyze_out_dir, exist_ok=True)\n",
    "\n",
    "    # 创建线程池\n",
    "    with ThreadPoolExecutor(max_workers=len(api_keys)) as executor:\n",
    "        # 提交错误日志分析任务\n",
    "        error_futures = []\n",
    "        for i in range(0, len(error_logs), 10):\n",
    "            batch = error_logs[i:i + 10]\n",
    "            selected_key = api_keys[i % len(api_keys)]\n",
    "            error_futures.append(\n",
    "                executor.submit(analyze_batch, batch, \"错误\", selected_key))\n",
    "\n",
    "        # 提交未知日志分析任务\n",
    "        unknown_futures = []\n",
    "        for i in range(0, len(unkonwn_logs), 10):\n",
    "            batch = unkonwn_logs[i:i + 10]\n",
    "            selected_key = api_keys[i % len(api_keys)]\n",
    "            unknown_futures.append(\n",
    "                executor.submit(analyze_batch, batch, \"未知\", selected_key))\n",
    "\n",
    "        # 收集并保存结果\n",
    "        results = []\n",
    "        for future in as_completed(error_futures + unknown_futures):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "            # 为每个批次创建单独的文件\n",
    "            timestamp = int(time.time())\n",
    "            filename = f\"{result['type']}_analysis_{timestamp}.txt\"\n",
    "            filepath = os.path.join(analyze_out_dir, filename)\n",
    "\n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                if 'analysis' in result:\n",
    "                    f.write(f\"=== {result['type']}日志分析结果 ===\\n\")\n",
    "                    f.write(f\"分析批次:\\n{result['batch']}\\n\\n\")\n",
    "                    f.write(f\"分析结果:\\n{result['analysis']}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"=== {result['type']}日志分析失败 ===\\n\")\n",
    "                    f.write(f\"错误信息:\\n{result['error']}\\n\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze(PROMPT_STRATEGIES, INPUT_FILE, raw_file_name, API_URL, API_KEYS,\n",
    "            analyze_log_directory):\n",
    "\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    if PROMPT_STRATEGIES == 'CoT':\n",
    "        df = df.sample(frac=1).reset_index(drop=True)\n",
    "        # answer_desc=\"a binary choice between normal and abnormal\"\n",
    "        prompt_header = \"Classify the given log entries into normal and abnormal categories. Do it with these steps: \\\n",
    "        (a) Mark it normal when values (such as memory address, floating number and register value) in a log are invalid. \\\n",
    "        (b) Mark it normal when lack of information. (c) Never consider <*> and missing values as abnormal patterns. \\\n",
    "        (d) Mark it abnormal when and only when the alert is explicitly expressed in textual content (such as keywords like error or interrupt). \\\n",
    "        Common label prompts do not explan and cannot be skipped. Organize your answer to be the following format: (x,y) x is log index and y is a binary choice between normal and abnormal \\\n",
    "        There are !!NumberControl!! logs, the logs begin: \"\n",
    "\n",
    "        prompt_header = '''You are a log anomaly classifier.\n",
    "\n",
    "        You will be given a list of log entries, each with a unique index.  \n",
    "        Your task is to determine whether each log is **abnormal (1)** or **normal (0)**.  \n",
    "        Only output in the following format, without any extra explanation or comments.\n",
    "\n",
    "        ## Output Format:\n",
    "        (log_idx, status)\n",
    "\n",
    "        ## Output Rules:\n",
    "        - status must be 1 if the log entry indicates an error, failure, crash, or unusual behavior.\n",
    "        - status must be 0 if the log entry is a normal operation or informational message.\n",
    "        - Output **only** a list of tuples. No extra text or explanation.\n",
    "\n",
    "        ## Input Logs:\n",
    "        '''\n",
    "\n",
    "        logs = df['log'].tolist()\n",
    "\n",
    "        ########## generate prompts ######################\n",
    "        prompt_parts, prompt_parts_count, log_parts = generate_prompt(\n",
    "            prompt_header, logs, max_len=5000)\n",
    "        ########### obtain raw answers from GPT ###########\n",
    "        lst = parse_logs(API_KEYS, API_URL, prompt_parts, prompt_parts_count,\n",
    "                         log_parts, raw_file_name)\n",
    "        ######### Align each log with its results #######\n",
    "        df_raw_answer = pd.read_excel(raw_file_name)\n",
    "        OUT_raw_path = write_to_excel(raw_file_name, df_raw_answer, logs)\n",
    "        unkonwn_logs, error_logs, unknown_error_logs = read_error_logs(\n",
    "            OUT_raw_path)\n",
    "        results = anylysis_error_logs(unkonwn_logs, error_logs, API_KEYS,\n",
    "                                      API_URL, analyze_log_directory)\n",
    "        return results, {\n",
    "            'unkonwn_logs': unkonwn_logs,\n",
    "            'error_logs': error_logs,\n",
    "            'unknown_error_logs': unknown_error_logs\n",
    "        }\n",
    "\n",
    "    if PROMPT_STRATEGIES == \"Self\":\n",
    "        #candidate selection\n",
    "        df = df[:100]\n",
    "        prompt_candidates = []\n",
    "        with open(\n",
    "                '/Users/hy_mbp/PycharmProjects/LogDetect/Find_detect/prompt_candidates.txt'\n",
    "        ) as f:\n",
    "            for line in f.readlines():\n",
    "                prompt_candidates.append(line.strip('\\n'))\n",
    "        for i, prompt_candidate in tqdm(enumerate(prompt_candidates)):\n",
    "            print('prompt %d' % (i + 1))\n",
    "            answer_desc = \"a parsed log template\"\n",
    "            prompt_header = \"%s Organize your answer to be the following format: !!FormatControl!!, where x is %s. There are !!NumberControl!! logs, the logs begin: \" % (\n",
    "                prompt_candidate, answer_desc)\n",
    "            logs = df['log'].tolist()\n",
    "            ########### generate prompts ######################\n",
    "            prompt_parts, prompt_parts_count, log_parts = generate_prompt(\n",
    "                prompt_header, logs, max_len=3000, no_reason=True)\n",
    "            ########### obtain raw answers from GPT ###########\n",
    "            # print(prompt_parts)\n",
    "            # print(prompt_parts_count)\n",
    "            # lts = parse_logs(API_KEYS,API_URL,prompt_parts,prompt_parts_count,'Candidate_%d_'%(i+1)+raw_file_name)\n",
    "            ########## Align each log with its results #######\n",
    "            # df_raw_answer = pd.read_excel(raw_file_name)\n",
    "            # write_to_excel(raw_file_name+'Candidate_%d_'%(i+1)+'.xlsx',df_raw_answer,logs,'sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx',API_URL)\n",
    "        return None\n",
    "\n",
    "\n",
    "# analyze('CoT','/Users/hy_mbp/PycharmProjects/LogDetect/log/OUTPUT_FILE/kernel.xlsx','/Users/hy_mbp/PycharmProjects/temp/raw_file_name1.xlsx','','')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def UpLoad_File(dir_path):\n",
    "    file_ls = []\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        root_file_ls = [os.path.join(root, file) for file in files]\n",
    "        for file in root_file_ls:\n",
    "            file_ls.append(file)\n",
    "    # 过滤 .DS_Store 文件\n",
    "    file_ls = [file for file in file_ls if not file.endswith('.DS_Store')]\n",
    "    return file_ls\n",
    "\n",
    "\n",
    "def convert_log_to_excel(DIR_path):\n",
    "    # 统一输出目录（避免路径拼接错误）\n",
    "    OUTPUT_DIR = os.path.join(DIR_path, 'OUTPUT_FILE')\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "    file_ls = UpLoad_File(DIR_path)\n",
    "    for file in file_ls:\n",
    "        # 生成安全的 Excel 文件名（替换路径中的斜杠为下划线）\n",
    "        safe_filename = os.path.basename(file).replace('/', '_')\n",
    "        safe_filename = safe_filename.replace(\n",
    "            '.', '_') + '.xlsx'  # 替换 Windows 路径分隔符\n",
    "        OUTPUT_EXCEL_PATH = os.path.join(OUTPUT_DIR, safe_filename)\n",
    "\n",
    "        # 读取日志文件（兼容非 UTF-8 编码，忽略无法解码的字符）\n",
    "        try:\n",
    "            with open(file, 'r', encoding='gbk', errors='ignore') as f:\n",
    "                logs = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        except Exception as e:\n",
    "            print(f\"警告：文件 {file} 读取失败，错误：{str(e)}，跳过处理。\")\n",
    "            continue\n",
    "\n",
    "        # 保存为 Excel\n",
    "        df = pd.DataFrame({'log': logs})\n",
    "        df.to_excel(OUTPUT_EXCEL_PATH, index=False)\n",
    "        print(f\"转换完成！Excel 文件已保存至：{OUTPUT_EXCEL_PATH}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 设置默认编码为UTF-8，避免Windows下的GBK编码问题\n",
    "    import sys\n",
    "    import io\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer,\n",
    "                                  encoding='utf-8',\n",
    "                                  errors='replace')\n",
    "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer,\n",
    "                                  encoding='utf-8',\n",
    "                                  errors='replace')\n",
    "\n",
    "    API_URL = \"https://api.siliconflow.cn/v1/chat/completions\"\n",
    "    API_KEYS = [\n",
    "        \"sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx\",\n",
    "        \"sk-kbucebwhrsoimqttlosgtxncyvmuvdyioncbadavayiovrns\",\n",
    "        \"sk-zzuykfebwbxkurfftzuvujqpwqzxxljegnxhhwtzddvwiigf\",\n",
    "        \"sk-qgsqryixuqdmtzkgubxpvdzollysgtonnvcrwmikwegmaogn\",\n",
    "        \"sk-hvxqvahoplbhdadwtaomisdamxqhquvummcfpvlafeovpqus\",\n",
    "    ]\n",
    "    INPUT_DIR = '../log/OUTPUT_FILE'\n",
    "    OUTPUT_DIR = '../Find_detect/output_528'\n",
    "    PROMPT_STRATEGIES = 'CoT'\n",
    "    analyze_log_path = '../Find_detect/output_528'\n",
    "    file_list = UpLoad_File(INPUT_DIR)\n",
    "    # file_list = file_list[:1]   # debug\n",
    "    Results = []\n",
    "    Error_logs = []\n",
    "    for file in file_list:\n",
    "        INPUT_FILE = file\n",
    "        raw_file_name = os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            os.path.basename(file).replace('.xlsx', '_raw.xlsx'))\n",
    "        print(raw_file_name)\n",
    "        results, error_log = analyze(PROMPT_STRATEGIES, INPUT_FILE,\n",
    "                                     raw_file_name, API_URL, API_KEYS,\n",
    "                                     analyze_log_path)\n",
    "        Results.append(results)\n",
    "        Error_logs.append(error_log)\n",
    "    # 根据每个日志的分析result和error_logs，再次调用最终模型完成最终的分析\n",
    "    error_logs = []\n",
    "    for i in range(len(Error_logs)):\n",
    "        error_logs.extend(Error_logs[i]['unknown_error_logs'])\n",
    "    error_logs = '\\n'.join(error_logs)\n",
    "    print(error_logs)\n",
    "\n",
    "    # 将error_logs保存为txt文件\n",
    "    # error_logs_file = os.path.join(OUTPUT_DIR, 'error_logs.txt')\n",
    "    # with open(error_logs_file, 'w', encoding='utf-8') as f:\n",
    "    #     f.write(error_logs)\n",
    "    # print(f\"错误日志已保存至: {error_logs_file}\")\n",
    "\n",
    "    analyze_log_final.analyze_log_directory(error_logs, option='str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Find_detect/output_528/log_czp_db1_kernel_raw.xlsx\n",
      "../Find_detect/output_528/log_database_log_raw.xlsx\n",
      "../Find_detect/output_528/log_log_controller_0_Event_txt_raw.xlsx\n",
      "../Find_detect/output_528/log_czp_db2_kernel_raw.xlsx\n",
      "../Find_detect/output_528/log_czp_db2_auth_raw.xlsx\n",
      "../Find_detect/output_528/log_resampled_sample_txt_raw.xlsx\n",
      "../Find_detect/output_528/log_czp_db1_auth_raw.xlsx\n",
      "../Find_detect/output_528/log_czp_db1_messages_raw.xlsx\n",
      "../Find_detect/output_528/log_czp_db2_messages_raw.xlsx\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "\n",
    "\n",
    "API_URL = \"https://api.siliconflow.cn/v1/chat/completions\"\n",
    "API_KEYS = [\n",
    "    \"sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx\",\n",
    "    \"sk-kbucebwhrsoimqttlosgtxncyvmuvdyioncbadavayiovrns\",\n",
    "    \"sk-zzuykfebwbxkurfftzuvujqpwqzxxljegnxhhwtzddvwiigf\",\n",
    "    \"sk-qgsqryixuqdmtzkgubxpvdzollysgtonnvcrwmikwegmaogn\",\n",
    "    \"sk-hvxqvahoplbhdadwtaomisdamxqhquvummcfpvlafeovpqus\",\n",
    "]\n",
    "INPUT_DIR = '../log/OUTPUT_FILE'\n",
    "OUTPUT_DIR = '../Find_detect/output_528'\n",
    "PROMPT_STRATEGIES = 'CoT'\n",
    "analyze_log_path = '../Find_detect/output_528'\n",
    "file_list = UpLoad_File(INPUT_DIR)\n",
    "# file_list = file_list[:1]   # debug\n",
    "Results = []\n",
    "Error_logs = []\n",
    "for file in file_list:\n",
    "    INPUT_FILE = file\n",
    "    raw_file_name = os.path.join(\n",
    "        OUTPUT_DIR,\n",
    "        os.path.basename(file).replace('.xlsx', '_raw.xlsx'))\n",
    "    print(raw_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78aadf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apr 23 14:43:31 czp-db1 kernel:  rport-15:0-18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apr 23 14:43:31 czp-db1 kernel: lpfc 0000:33:0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 log\n",
       "1  Apr 23 14:43:31 czp-db1 kernel:  rport-15:0-18...\n",
       "2  Apr 23 14:43:31 czp-db1 kernel: lpfc 0000:33:0...\n",
       "3  Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: r...\n",
       "4  Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [...\n",
       "5  Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "df = pd.read_excel(file_list[0]).iloc[1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e43d374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "\n",
    "def run_prompt(prompt: str, api_key: str, api_url: str) -> str:\n",
    "    \"\"\"\n",
    "    调用大模型 API，输入 prompt，返回回答内容。\n",
    "\n",
    "    参数：\n",
    "    - prompt: 输入的文本提示\n",
    "    - api_key: API 密钥（string）\n",
    "    - api_url: 模型服务的 URL（string）\n",
    "\n",
    "    返回：\n",
    "    - 模型输出的内容（string）\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"THUDM/GLM-4-9B-0414\",  # 可根据需要替换为 \"Qwen/Qwen3-8B\" 等模型名\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 8192,\n",
    "        \"enable_thinking\": True,\n",
    "        \"thinking_budget\": 4096,\n",
    "        \"min_p\": 0.05,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.3,\n",
    "        \"top_k\": 20,\n",
    "        \"frequency_penalty\": 0.2,\n",
    "        \"n\": 1,\n",
    "        \"stop\": [],\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(api_url, json=payload, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "\n",
    "def build_prompt(df, prompt_header: str, max_len: int = 40000) -> list:\n",
    "    \"\"\"\n",
    "    将 df['log'] 构建为一个或多个符合最大长度限制的 prompt。\n",
    "\n",
    "    参数：\n",
    "    - df: 包含日志的 DataFrame，要求含有列 df['log']\n",
    "    - prompt_header: prompt 开头说明文本（如 log classifier）\n",
    "    - max_len: 每个 prompt 的最大字符数（默认 4000）\n",
    "\n",
    "    返回：\n",
    "    - prompt_list: List[str]，每个元素是一条完整的 prompt\n",
    "    \"\"\"\n",
    "    prompt_list = []\n",
    "    current_prompt = prompt_header.strip() + \"\\n\"\n",
    "    current_logs = []\n",
    "    for i, log in enumerate(df['log']):\n",
    "        log_entry = f\"({i}) {log}\"\n",
    "        if len(current_prompt) + len(log_entry) + 1 <= max_len:\n",
    "            current_prompt += log_entry + \"\\n\"\n",
    "        else:\n",
    "            prompt_list.append(current_prompt.strip())\n",
    "            current_prompt = prompt_header.strip() + \"\\n\" + log_entry + \"\\n\"\n",
    "\n",
    "    # 最后一批日志加入\n",
    "    if current_prompt.strip() != prompt_header.strip():\n",
    "        prompt_list.append(current_prompt.strip())\n",
    "    return prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a360ec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompt_header = '''You are a log anomaly classifier.\n",
    "\n",
    "You will be given a list of log entries, each with a unique index.  \n",
    "Your task is to determine whether each log is **abnormal (1)** or **normal (0)**.  \n",
    "Only output in the following format, without any extra explanation or comments.\n",
    "\n",
    "## Output Format:\n",
    "(log_idx, status)\n",
    "\n",
    "## Output Rules:\n",
    "- status must be 1 if the log entry indicates an error, failure, crash, or unusual behavior.\n",
    "- status must be 0 if the log entry is a normal operation or informational message.\n",
    "- Output **only** a list of tuples. No extra text or explanation.\n",
    "\n",
    "## Input Logs:\n",
    "'''\n",
    "\n",
    "prompt_list = build_prompt(df.iloc[:100], prompt_header)\n",
    "# for prompt in prompt_list:\n",
    "#     print(prompt)\n",
    "#     print(\"\\n\" + \"=\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d037dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d944f048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx',\n",
       " 'https://api.siliconflow.cn/v1/chat/completions')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_KEYS[0], API_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "775c3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a log anomaly classifier.\n",
      "\n",
      "You will be given a list of log entries, each with a unique index.  \n",
      "Your task is to determine whether each log is **abnormal (1)** or **normal (0)**.  \n",
      "Only output in the following format, without any extra explanation or comments.\n",
      "\n",
      "## Output Format:\n",
      "(log_idx, status)\n",
      "\n",
      "## Output Rules:\n",
      "- status must be 1 if the log entry indicates an error, failure, crash, or unusual behavior.\n",
      "- status must be 0 if the log entry is a normal operation or informational message.\n",
      "- Output **only** a list of tuples. No extra text or explanation.\n",
      "\n",
      "## Input Logs:\n",
      "(0) Apr 23 14:43:31 czp-db1 kernel:  rport-15:0-18: blocked FC remote port time out: removing target and saving binding\n",
      "(1) Apr 23 14:43:31 czp-db1 kernel: lpfc 0000:33:00.0: 0:(0):0203 Devloss timeout on WWPN 20:80:3c:78:43:c5:45:3e NPort x1f0400 Data: x0 x8 x0\n",
      "(2) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: rejecting I/O to offline device\n",
      "(3) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi] killing request\n",
      "(4) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK\n",
      "(5) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi] CDB: Write(10): 2a 00 2b 70 5f e8 00 00 08 00\n",
      "(6) Apr 23 14:43:31 czp-db1 kernel: end_request: I/O error, dev sdi, sector 728784872\n",
      "(7) Apr 23 14:43:31 czp-db1 kernel: device-mapper: multipath: Failing path 8:128.\n",
      "(8) Apr 23 15:59:09 czp-db1 kernel: klogd 1.5.0, log source = /proc/kmsg started.\n",
      "(9) Apr 23 15:59:09 czp-db1 kernel: Inspecting /boot/System.map-2.6.32.700-Rocky4.2-x86_64\n",
      "(10) Apr 23 15:59:09 czp-db1 kernel: Inspecting /usr/src/linux/System.map\n",
      "(11) Apr 23 15:59:10 czp-db1 kernel: Cannot find map file.\n",
      "(12) Apr 23 15:59:10 czp-db1 kernel: Loaded 72722 symbols from 60 modules.\n",
      "(13) Apr 23 15:59:10 czp-db1 kernel: Initializing cgroup subsys cpuset\n",
      "(14) Apr 23 15:59:10 czp-db1 kernel: Initializing cgroup subsys cpu\n",
      "(15) Apr 23 15:59:10 czp-db1 kernel: Linux version 2.6.32.700-Rocky4.2-x86_64 (root@Auto-builder) (gcc version 4.4.5 (Linx 4.4.5-8linx1) ) #1 SMP Mon May 7 13:51:25 CST 2018\n",
      "(16) Apr 23 15:59:10 czp-db1 kernel: Command line: BOOT_IMAGE=/boot/vmlinuz-2.6.32.700-Rocky4.2-x86_64 root=UUID=b9b725ae-b50e-4fe6-9373-e139932cbdf4 ro linx_serial=FCA5FEE48C15E12E security=linx rootflags=data=writeback elevator=deadline quiet acpi=force\n",
      "(17) Apr 23 15:59:10 czp-db1 kernel: KERNEL supported cpus:\n",
      "(18) Apr 23 15:59:10 czp-db1 kernel:   Intel GenuineIntel\n",
      "(19) Apr 23 15:59:10 czp-db1 kernel:   AMD AuthenticAMD\n",
      "(20) Apr 23 15:59:10 czp-db1 kernel:   Centaur CentaurHauls\n",
      "(21) Apr 23 15:59:10 czp-db1 kernel: BIOS-provided physical RAM map:\n",
      "(22) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000000000000 - 000000000009a000 (usable)\n",
      "(23) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000000009a000 - 00000000000a0000 (reserved)\n",
      "(24) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000000e0000 - 0000000000100000 (reserved)\n",
      "(25) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000000100000 - 000000006a52a000 (usable)\n",
      "(26) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006a52a000 - 000000006c62a000 (reserved)\n",
      "(27) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006c62a000 - 000000006c73a000 (usable)\n",
      "(28) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006c73a000 - 000000006d328000 (ACPI NVS)\n",
      "(29) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006d328000 - 000000006f232000 (reserved)\n",
      "(30) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006f232000 - 000000006f800000 (usable)\n",
      "(31) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006f800000 - 0000000090000000 (reserved)\n",
      "(32) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000fd000000 - 00000000fe800000 (reserved)\n",
      "(33) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000fed20000 - 00000000fed45000 (reserved)\n",
      "(34) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000ff000000 - 0000000100000000 (reserved)\n",
      "(35) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000100000000 - 0000004080000000 (usable)\n",
      "(36) Apr 23 15:59:10 czp-db1 kernel: SMBIOS version 3.0 @ 0xF05E0\n",
      "(37) Apr 23 15:59:10 czp-db1 kernel: SMBIOS 3.0 present.\n",
      "(38) Apr 23 15:59:10 czp-db1 kernel: DMI: Sugon I840-G30/80P48-US, BIOS 0THSY016 04/16/2018\n",
      "(39) Apr 23 15:59:10 czp-db1 kernel: AMI BIOS detected: BIOS may corrupt low RAM, working around it.\n",
      "(40) Apr 23 15:59:10 czp-db1 kernel: e820 update range: 0000000000000000 - 0000000000010000 (usable) ==> (reserved)\n",
      "(41) Apr 23 15:59:10 czp-db1 kernel: e820 update range: 0000000000000000 - 0000000000001000 (usable) ==> (reserved)\n",
      "(42) Apr 23 15:59:10 czp-db1 kernel: e820 remove range: 00000000000a0000 - 0000000000100000 (usable)\n",
      "(43) Apr 23 15:59:10 czp-db1 kernel: last_pfn = 0x4080000 max_arch_pfn = 0x400000000\n",
      "(44) Apr 23 15:59:10 czp-db1 kernel: x86 PAT enabled: cpu 0, old 0x7040600070406, new 0x7010600070106\n",
      "(45) Apr 23 15:59:10 czp-db1 kernel: original variable MTRRs\n",
      "(46) Apr 23 15:59:10 czp-db1 kernel: reg 0, base: 0GB, range: 256GB, type WB\n",
      "(47) Apr 23 15:59:10 czp-db1 kernel: reg 1, base: 256GB, range: 2GB, type WB\n",
      "(48) Apr 23 15:59:10 czp-db1 kernel: reg 2, base: 2GB, range: 2GB, type UC\n",
      "(49) Apr 23 15:59:10 czp-db1 kernel: reg 3, base: 2032MB, range: 16MB, type UC\n",
      "(50) Apr 23 15:59:10 czp-db1 kernel: total RAM covered: 262128M\n",
      "(51) Apr 23 15:59:10 czp-db1 kernel: Found optimal setting for mtrr clean up\n",
      "(52) Apr 23 15:59:10 czp-db1 kernel:  gran_size: 64K ^Ichunk_size: 256G ^Inum_reg: 4  ^Ilose cover RAM: 0G\n",
      "(53) Apr 23 15:59:10 czp-db1 kernel: New variable MTRRs\n",
      "(54) Apr 23 15:59:10 czp-db1 kernel: reg 0, base: 0GB, range: 256GB, type WB\n",
      "(55) Apr 23 15:59:10 czp-db1 kernel: reg 1, base: 2032MB, range: 16MB, type UC\n",
      "(56) Apr 23 15:59:10 czp-db1 kernel: reg 2, base: 2GB, range: 2GB, type UC\n",
      "(57) Apr 23 15:59:10 czp-db1 kernel: reg 3, base: 256GB, range: 2GB, type WB\n",
      "(58) Apr 23 15:59:10 czp-db1 kernel: e820 update range: 000000007f000000 - 0000000100000000 (usable) ==> (reserved)\n",
      "(59) Apr 23 15:59:10 czp-db1 kernel: last_pfn = 0x6f800 max_arch_pfn = 0x400000000\n",
      "(60) Apr 23 15:59:10 czp-db1 kernel: initial memory mapped : 0 - 20000000\n",
      "(61) Apr 23 15:59:10 czp-db1 kernel: Using GB pages for direct mapping\n",
      "(62) Apr 23 15:59:10 czp-db1 kernel: init_memory_mapping: 0000000000000000-000000006f800000\n",
      "(63) Apr 23 15:59:10 czp-db1 kernel:  0000000000 - 0040000000 page 1G\n",
      "(64) Apr 23 15:59:10 czp-db1 kernel:  0040000000 - 006f800000 page 2M\n",
      "(65) Apr 23 15:59:10 czp-db1 kernel: kernel direct mapping tables up to 6f800000 @ 10000-12000\n",
      "(66) Apr 23 15:59:10 czp-db1 kernel: Use unified mapping for non-reserved e820 regions.\n",
      "(67) Apr 23 15:59:10 czp-db1 kernel: init_memory_mapping: 0000000100000000-0000004080000000\n",
      "(68) Apr 23 15:59:10 czp-db1 kernel:  0100000000 - 4080000000 page 1G\n",
      "(69) Apr 23 15:59:10 czp-db1 kernel: kernel direct mapping tables up to 4080000000 @ 11000-12000\n",
      "(70) Apr 23 15:59:10 czp-db1 kernel: RAMDISK: 376e1000 - 37fef88a\n",
      "(71) Apr 23 15:59:10 czp-db1 kernel: ACPI: Deleted _OSI(Windows 2012)\n",
      "(72) Apr 23 15:59:10 czp-db1 kernel: ACPI: Deleted _OSI(Windows 2013)\n",
      "(73) Apr 23 15:59:10 czp-db1 kernel: ACPI: RSDP 00000000000f05b0 00024 (v02 ALASKA)\n",
      "(74) Apr 23 15:59:10 czp-db1 kernel: ACPI: XSDT 000000006c73a0c8 00114 (v01 ALASKA   A M I  01072009 AMI  00010013)\n",
      "(75) Apr 23 15:59:10 czp-db1 kernel: ACPI: FACP 000000006c783ca0 00114 (v06 ALASKA   A M I  01072009 INTL 20091013)\n",
      "(76) Apr 23 15:59:10 czp-db1 kernel: ACPI: DSDT 000000006c73a278 49A28 (v02 ALASKA   A M I  01072009 INTL 20091013)\n",
      "(77) Apr 23 15:59:10 czp-db1 kernel: ACPI: FACS 000000006d326080 00040\n",
      "(78) Apr 23 15:59:10 czp-db1 kernel: ACPI: FPDT 000000006c783db8 00044 (v01 ALASKA   A M I  01072009 AMI  00010013)\n",
      "(79) Apr 23 15:59:10 czp-db1 kernel: ACPI: FIDT 000000006c783e00 0009C (v01 ALASKA    A M I 01072009 AMI  00010013)\n",
      "(80) Apr 23 15:59:10 czp-db1 kernel: ACPI: UEFI 000000006c783ea0 0005C (v01  INTEL RstUefiV 00000000      00000000)\n",
      "(81) Apr 23 15:59:10 czp-db1 kernel: ACPI: UEFI 000000006c783f00 00042 (v01 ALASKA   A M I  01072009      01000013)\n",
      "(82) Apr 23 15:59:10 czp-db1 kernel: ACPI: MCFG 000000006c783f48 0003C (v01 ALASKA    A M I 01072009 MSFT 00000097)\n",
      "(83) Apr 23 15:59:10 czp-db1 kernel: ACPI: HPET 000000006c783f88 00038 (v01 ALASKA   A M I  00000001 INTL 20091013)\n",
      "(84) Apr 23 15:59:10 czp-db1 kernel: ACPI: APIC 000000006c783fc0 016DE (v03 ALASKA   A M I  00000000 INTL 20091013)\n",
      "(85) Apr 23 15:59:10 czp-db1 kernel: ACPI: MIGT 000000006c7856a0 00040 (v01 ALASKA   A M I  00000000 INTL 20091013)\n",
      "(86) Apr 23 15:59:10 czp-db1 kernel: ACPI: MSCT 000000006c7856e0 00090 (v01 ALASKA   A M I  00000001 INTL 20091013)\n",
      "(87) Apr 23 15:59:10 czp-db1 kernel: ACPI: NFIT 000000006c785770 18028 (v01                 00000000      00000000)\n",
      "(88) Apr 23 15:59:10 czp-db1 kernel: ACPI: PCAT 000000006c79d798 00048 (v01 ALASKA   A M I  00000002 INTL 20091013)\n",
      "(89) Apr 23 15:59:10 czp-db1 kernel: ACPI: PCCT 000000006c79d7e0 0006E (v01 ALASKA   A M I  00000002 INTL 20091013)\n",
      "(90) Apr 23 15:59:10 czp-db1 kernel: ACPI: RASF 000000006c79d850 00030 (v01 ALASKA   A M I  00000001 INTL 20091013)\n",
      "(91) Apr 23 15:59:10 czp-db1 kernel: ACPI: SLIT 000000006c79d880 0003C (v01 ALASKA   A M I  00000001 INTL 20091013)\n",
      "(92) Apr 23 15:59:10 czp-db1 kernel: ACPI: SRAT 000000006c79d8c0 02830 (v03 ALASKA   A M I  00000002 INTL 20091013)\n",
      "(93) Apr 23 15:59:10 czp-db1 kernel: ACPI: SVOS 000000006c7a00f0 00032 (v01 ALASKA   A M I  00000000 INTL 20091013)\n",
      "(94) Apr 23 15:59:10 czp-db1 kernel: ACPI: WDDT 000000006c7a0128 00040 (v01 ALASKA   A M I  00000000 INTL 20091013)\n",
      "(95) Apr 23 15:59:10 czp-db1 kernel: ACPI: OEM4 000000006c7a0168 A27C4 (v02  INTEL CPU  CST 00003000 INTL 20140828)\n",
      "(96) Apr 23 15:59:10 czp-db1 kernel: ACPI: OEM1 000000006c842930 2A2C4 (v02  INTEL CPU EIST 00003000 INTL 20140828)\n",
      "(97) Apr 23 15:59:10 czp-db1 kernel: ACPI: OEM2 000000006c86cbf8 19464 (v02  INTEL CPU  HWP 00003000 INTL 20140828)\n",
      "(98) Apr 23 15:59:10 czp-db1 kernel: ACPI: SSDT 000000006c886060 33990 (v02  INTEL SSDT  PM 00004000 INTL 20140828)\n",
      "(99) Apr 23 15:59:10 czp-db1 kernel: ACPI: SSDT 000000006c8b99f0 0065B (v02 ALASKA   A M I  00000000 INTL 20091013)\n"
     ]
    }
   ],
   "source": [
    "print(prompt_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc220400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(0, 1)\\n(1, 1)\\n(2, 1)\\n(3, 1)\\n(4, 0)\\n(5, 0)\\n(6, 1)\\n(7, 1)\\n(8, 0)\\n(9, 0)\\n(10, 0)\\n(11, 1)\\n(12, 0)\\n(13, 0)\\n(14, 0)\\n(15, 0)\\n(16, 0)\\n(17, 0)\\n(18, 0)\\n(19, 0)\\n(20, 0)\\n(21, 0)\\n(22, 0)\\n(23, 0)\\n(24, 0)\\n(25, 0)\\n(26, 0)\\n(27, 0)\\n(28, 0)\\n(29, 0)\\n(30, 0)\\n(31, 0)\\n(32, 0)\\n(33, 0)\\n(34, 0)\\n(35, 0)\\n(36, 0)\\n(37, 0)\\n(38, 0)\\n(39, 0)\\n(40, 0)\\n(41, 0)\\n(42, 0)\\n(43, 0)\\n(44, 0)\\n(45, 0)\\n(46, 0)\\n(47, 0)\\n(48, 0)\\n(49, 0)\\n(50, 0)\\n(51, 0)\\n(52, 0)\\n(53, 0)\\n(54, 0)\\n(55, 0)\\n(56, 0)\\n(57, 0)\\n(58, 0)\\n(59, 0)\\n(60, 0)\\n(61, 0)\\n(62, 0)\\n(63, 0)\\n(64, 0)\\n(65, 0)\\n(66, 0)\\n(67, 0)\\n(68, 0)\\n(69, 0)\\n(70, 0)\\n(71, 0)\\n(72, 0)\\n(73, 0)\\n(74, 0)\\n(75, 0)\\n(76, 0)\\n(77, 0)\\n(78, 0)\\n(79, 0)\\n(80, 0)\\n(81, 0)\\n(82, 0)\\n(83, 0)\\n(84, 0)\\n(85, 0)\\n(86, 0)\\n(87, 0)\\n(88, 0)\\n(89, 0)\\n(90, 0)\\n(91, 0)\\n(92, 0)\\n(93, 0)\\n(94, 0)\\n(95, 0)\\n(96, 0)\\n(97, 0)\\n(98, 0)\\n(99, 0)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_prompt(prompt_list[0], API_KEYS[0], API_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccf444a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a log anomaly classifier.\\n\\nYou will be given a list of log entries, each with a unique index.  \\nYour task is to determine whether each log is **abnormal (1)** or **normal (0)**.  \\nOnly output in the following format, without any extra explanation or comments.\\n\\n## Output Format:\\n(log_idx, status)\\n\\n## Output Rules:\\n- status must be 1 if the log entry indicates an error, failure, crash, or unusual behavior.\\n- status must be 0 if the log entry is a normal operation or informational message.\\n- Output **only** a list of tuples. No extra text or explanation.\\n\\n## Input Logs:\\n(0) Apr 23 14:43:31 czp-db1 kernel:  rport-15:0-18: blocked FC remote port time out: removing target and saving binding\\n(1) Apr 23 14:43:31 czp-db1 kernel: lpfc 0000:33:00.0: 0:(0):0203 Devloss timeout on WWPN 20:80:3c:78:43:c5:45:3e NPort x1f0400 Data: x0 x8 x0\\n(2) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: rejecting I/O to offline device\\n(3) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi] killing request\\n(4) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi]  Result: hostbyte=DID_NO_CONNECT driverbyte=DRIVER_OK\\n(5) Apr 23 14:43:31 czp-db1 kernel: sd 15:0:3:2: [sdi] CDB: Write(10): 2a 00 2b 70 5f e8 00 00 08 00\\n(6) Apr 23 14:43:31 czp-db1 kernel: end_request: I/O error, dev sdi, sector 728784872\\n(7) Apr 23 14:43:31 czp-db1 kernel: device-mapper: multipath: Failing path 8:128.\\n(8) Apr 23 15:59:09 czp-db1 kernel: klogd 1.5.0, log source = /proc/kmsg started.\\n(9) Apr 23 15:59:09 czp-db1 kernel: Inspecting /boot/System.map-2.6.32.700-Rocky4.2-x86_64\\n(10) Apr 23 15:59:09 czp-db1 kernel: Inspecting /usr/src/linux/System.map\\n(11) Apr 23 15:59:10 czp-db1 kernel: Cannot find map file.\\n(12) Apr 23 15:59:10 czp-db1 kernel: Loaded 72722 symbols from 60 modules.\\n(13) Apr 23 15:59:10 czp-db1 kernel: Initializing cgroup subsys cpuset\\n(14) Apr 23 15:59:10 czp-db1 kernel: Initializing cgroup subsys cpu\\n(15) Apr 23 15:59:10 czp-db1 kernel: Linux version 2.6.32.700-Rocky4.2-x86_64 (root@Auto-builder) (gcc version 4.4.5 (Linx 4.4.5-8linx1) ) #1 SMP Mon May 7 13:51:25 CST 2018\\n(16) Apr 23 15:59:10 czp-db1 kernel: Command line: BOOT_IMAGE=/boot/vmlinuz-2.6.32.700-Rocky4.2-x86_64 root=UUID=b9b725ae-b50e-4fe6-9373-e139932cbdf4 ro linx_serial=FCA5FEE48C15E12E security=linx rootflags=data=writeback elevator=deadline quiet acpi=force\\n(17) Apr 23 15:59:10 czp-db1 kernel: KERNEL supported cpus:\\n(18) Apr 23 15:59:10 czp-db1 kernel:   Intel GenuineIntel\\n(19) Apr 23 15:59:10 czp-db1 kernel:   AMD AuthenticAMD\\n(20) Apr 23 15:59:10 czp-db1 kernel:   Centaur CentaurHauls\\n(21) Apr 23 15:59:10 czp-db1 kernel: BIOS-provided physical RAM map:\\n(22) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000000000000 - 000000000009a000 (usable)\\n(23) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000000009a000 - 00000000000a0000 (reserved)\\n(24) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000000e0000 - 0000000000100000 (reserved)\\n(25) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000000100000 - 000000006a52a000 (usable)\\n(26) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006a52a000 - 000000006c62a000 (reserved)\\n(27) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006c62a000 - 000000006c73a000 (usable)\\n(28) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006c73a000 - 000000006d328000 (ACPI NVS)\\n(29) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006d328000 - 000000006f232000 (reserved)\\n(30) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006f232000 - 000000006f800000 (usable)\\n(31) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 000000006f800000 - 0000000090000000 (reserved)\\n(32) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000fd000000 - 00000000fe800000 (reserved)\\n(33) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000fed20000 - 00000000fed45000 (reserved)\\n(34) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 00000000ff000000 - 0000000100000000 (reserved)\\n(35) Apr 23 15:59:10 czp-db1 kernel:  BIOS-e820: 0000000100000000 - 0000004080000000 (usable)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_list[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
