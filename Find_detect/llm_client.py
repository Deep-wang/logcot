import requests
import json
from tenacity import retry, stop_after_attempt, wait_exponential
import copy


class LLMClient:
    """
    å¤§æ¨¡å‹å®¢æˆ·ç«¯ç±»ï¼Œç”¨äºè°ƒç”¨APIæ¥å£
    æ”¯æŒå•æ¬¡è°ƒç”¨å’Œå¤šè½®å¯¹è¯åŠŸèƒ½
    """
    
    def __init__(
        self,
        api_url="https://api.siliconflow.cn/v1/chat/completions",
        api_key="sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx",
        model="THUDM/GLM-4-9B-0414",
        stream=False,
        max_tokens=8192,
        enable_thinking=True,
        thinking_budget=4096,
        min_p=0.1,
        temperature=0.1,
        top_p=0.3,
        top_k=20,
        frequency_penalty=0.1,
        presence_penalty=0.0,
        n=1,
        stop=None,
        retry_attempts=7,
        retry_wait_multiplier=2,
        retry_wait_min=2,
        retry_wait_max=100,
        max_history_length=50,  # æœ€å¤§å†å²å¯¹è¯è½®æ•°
        auto_truncate=True      # æ˜¯å¦è‡ªåŠ¨æˆªæ–­è¿‡é•¿çš„å¯¹è¯å†å²
    ):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            api_url: APIè°ƒç”¨åœ°å€
            api_key: APIå¯†é’¥
            model: æ¨¡å‹åç§°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            thinking_budget: æ€è€ƒé¢„ç®—tokenæ•°
            min_p: æœ€å°æ¦‚ç‡é˜ˆå€¼
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§
            top_p: æ ¸é‡‡æ ·å‚æ•°
            top_k: Top-Ké‡‡æ ·å‚æ•°
            frequency_penalty: é¢‘ç‡æƒ©ç½š
            presence_penalty: å­˜åœ¨æƒ©ç½š
            n: ç”Ÿæˆå›å¤æ•°é‡
            stop: åœæ­¢è¯åˆ—è¡¨
            retry_attempts: é‡è¯•æ¬¡æ•°
            retry_wait_multiplier: é‡è¯•ç­‰å¾…æ—¶é—´å€æ•°
            retry_wait_min: æœ€å°ç­‰å¾…æ—¶é—´
            retry_wait_max: æœ€å¤§ç­‰å¾…æ—¶é—´
            max_history_length: æœ€å¤§å†å²å¯¹è¯è½®æ•°
            auto_truncate: æ˜¯å¦è‡ªåŠ¨æˆªæ–­è¿‡é•¿çš„å¯¹è¯å†å²
        """
        self.api_url = api_url
        self.api_key = api_key
        self.model = model
        self.stream = stream
        self.max_tokens = max_tokens
        self.enable_thinking = enable_thinking
        self.thinking_budget = thinking_budget
        self.min_p = min_p
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.frequency_penalty = frequency_penalty
        self.presence_penalty = presence_penalty
        self.n = n
        self.stop = stop if stop is not None else []
        
        # é‡è¯•é…ç½®
        self.retry_attempts = retry_attempts
        self.retry_wait_multiplier = retry_wait_multiplier
        self.retry_wait_min = retry_wait_min
        self.retry_wait_max = retry_wait_max
        
        # å¤šè½®å¯¹è¯é…ç½®
        self.max_history_length = max_history_length
        self.auto_truncate = auto_truncate
        
        # å¯¹è¯å†å²å­˜å‚¨
        self.conversation_history = []
        self.system_prompt = None
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    @retry(stop=stop_after_attempt(7), wait=wait_exponential(multiplier=2, min=2, max=100))
    def _post_with_retry(self, payload):
        """
        å¸¦é‡è¯•æœºåˆ¶çš„POSTè¯·æ±‚
        
        Args:
            payload: è¯·æ±‚è½½è·
            
        Returns:
            dict: APIå“åº”
        """
        response = requests.post(self.api_url, json=payload, headers=self.headers)
        response.raise_for_status()
        return response.json()
    
    def forward(self, content, system_prompt=None, role="user"):
        """
        å‰å‘æ¨ç†ï¼Œè°ƒç”¨å¤§æ¨¡å‹è·å–å“åº”ï¼ˆå•æ¬¡è°ƒç”¨ï¼Œä¸ä¿å­˜å†å²ï¼‰
        
        Args:
            content: ç”¨æˆ·è¾“å…¥å†…å®¹
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            role: æ¶ˆæ¯è§’è‰²ï¼Œé»˜è®¤ä¸º"user"
            
        Returns:
            str: å¤§æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            Exception: è°ƒç”¨APIå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæä¾›ï¼‰
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": role,
            "content": content
        })
        
        return self._call_api(messages)
    
    def chat(self, content, role="user"):
        """
        å¤šè½®å¯¹è¯åŠŸèƒ½ï¼Œä¼šä¿å­˜å¯¹è¯å†å²
        
        Args:
            content: ç”¨æˆ·è¾“å…¥å†…å®¹
            role: æ¶ˆæ¯è§’è‰²ï¼Œé»˜è®¤ä¸º"user"
            
        Returns:
            str: å¤§æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬å†…å®¹
            
        Raises:
            Exception: è°ƒç”¨APIå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({
            "role": role,
            "content": content
        })
        
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        messages = self._build_messages_with_history()
        
        # è°ƒç”¨API
        response = self._call_api(messages)
        
        # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å†å²
        self.conversation_history.append({
            "role": "assistant",
            "content": response
        })
        
        # æ£€æŸ¥å†å²é•¿åº¦å¹¶è‡ªåŠ¨æˆªæ–­
        if self.auto_truncate:
            self._truncate_history()
        
        return response
    
    def _call_api(self, messages):
        """
        è°ƒç”¨APIçš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            str: APIå“åº”å†…å®¹
        """
        # æ„å»ºè¯·æ±‚è½½è·
        payload = {
            "model": self.model,
            "stream": self.stream,
            "max_tokens": self.max_tokens,
            "enable_thinking": self.enable_thinking,
            "thinking_budget": self.thinking_budget,
            "min_p": self.min_p,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            "n": self.n,
            "stop": self.stop,
            "messages": messages,
        }
        
        try:
            # è°ƒç”¨API
            response = self._post_with_retry(payload)
            # æå–å“åº”å†…å®¹
            result = response['choices'][0]['message']['content']
            return result
            
        except Exception as e:
            raise Exception(f"LLMè°ƒç”¨å¤±è´¥: {e}")
    
    def _build_messages_with_history(self):
        """
        æ„å»ºåŒ…å«å†å²å¯¹è¯çš„æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            list: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.system_prompt:
            messages.append({
                "role": "system",
                "content": self.system_prompt
            })
        
        # æ·»åŠ å¯¹è¯å†å²
        messages.extend(self.conversation_history)
        
        return messages
    
    def _truncate_history(self):
        """
        æˆªæ–­è¿‡é•¿çš„å¯¹è¯å†å²
        ä¿ç•™æœ€è¿‘çš„å¯¹è¯ï¼Œç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶
        """
        if len(self.conversation_history) > self.max_history_length:
            # ä¿ç•™æœ€è¿‘çš„å¯¹è¯
            excess_count = len(self.conversation_history) - self.max_history_length
            self.conversation_history = self.conversation_history[excess_count:]
            print(f"âš ï¸ å¯¹è¯å†å²å·²æˆªæ–­ï¼Œç§»é™¤äº† {excess_count} æ¡æ—©æœŸæ¶ˆæ¯")
    
    def set_system_prompt(self, system_prompt):
        """
        è®¾ç½®ç³»ç»Ÿæç¤ºè¯ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯å†…å®¹
        """
        self.system_prompt = system_prompt
        print(f"âœ… ç³»ç»Ÿæç¤ºè¯å·²è®¾ç½®")
    
    def clear_history(self):
        """
        æ¸…ç©ºå¯¹è¯å†å²
        """
        self.conversation_history = []
        print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
    
    def get_history(self, format_type="list"):
        """
        è·å–å¯¹è¯å†å²
        
        Args:
            format_type: è¿”å›æ ¼å¼ï¼Œ"list"è¿”å›åŸå§‹åˆ—è¡¨ï¼Œ"text"è¿”å›æ ¼å¼åŒ–æ–‡æœ¬
            
        Returns:
            list or str: å¯¹è¯å†å²
        """
        if format_type == "text":
            history_text = ""
            for i, msg in enumerate(self.conversation_history):
                role_emoji = "ğŸ§‘" if msg["role"] == "user" else "ğŸ¤–"
                history_text += f"{role_emoji} {msg['role'].title()}: {msg['content']}\n\n"
            return history_text
        else:
            return copy.deepcopy(self.conversation_history)
    
    def get_history_length(self):
        """
        è·å–å½“å‰å¯¹è¯å†å²é•¿åº¦
        
        Returns:
            int: å¯¹è¯å†å²ä¸­çš„æ¶ˆæ¯æ•°é‡
        """
        return len(self.conversation_history)
    
    def save_conversation(self, filepath):
        """
        ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶
        
        Args:
            filepath: ä¿å­˜æ–‡ä»¶è·¯å¾„
        """
        conversation_data = {
            "system_prompt": self.system_prompt,
            "conversation_history": self.conversation_history,
            "model": self.model,
            "timestamp": self._get_timestamp()
        }
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ°: {filepath}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¯¹è¯å¤±è´¥: {e}")
    
    def load_conversation(self, filepath):
        """
        ä»æ–‡ä»¶åŠ è½½å¯¹è¯
        
        Args:
            filepath: å¯¹è¯æ–‡ä»¶è·¯å¾„
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                conversation_data = json.load(f)
            
            self.system_prompt = conversation_data.get("system_prompt")
            self.conversation_history = conversation_data.get("conversation_history", [])
            
            print(f"ğŸ“‚ å¯¹è¯å·²ä» {filepath} åŠ è½½")
            print(f"ğŸ“Š åŠ è½½äº† {len(self.conversation_history)} æ¡å†å²æ¶ˆæ¯")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¯¹è¯å¤±è´¥: {e}")
    
    def _get_timestamp(self):
        """
        è·å–å½“å‰æ—¶é—´æˆ³
        
        Returns:
            str: æ ¼å¼åŒ–çš„æ—¶é—´æˆ³
        """
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def start_interactive_chat(self):
        """
        å¯åŠ¨äº¤äº’å¼èŠå¤©æ¨¡å¼
        """
        print("ğŸš€ è¿›å…¥äº¤äº’å¼èŠå¤©æ¨¡å¼")
        print("ğŸ’¡ è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("ğŸ’¡ è¾“å…¥ 'clear' æ¸…ç©ºå†å²")
        print("ğŸ’¡ è¾“å…¥ 'history' æŸ¥çœ‹å¯¹è¯å†å²")
        print("ğŸ’¡ è¾“å…¥ 'save <filename>' ä¿å­˜å¯¹è¯")
        print("=" * 50)
        
        while True:
            try:
                user_input = input("\nğŸ§‘ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    self.clear_history()
                    continue
                elif user_input.lower() == 'history':
                    history = self.get_history("text")
                    if history:
                        print("\nğŸ“‹ å¯¹è¯å†å²:")
                        print("-" * 30)
                        print(history)
                    else:
                        print("ğŸ“‹ æš‚æ— å¯¹è¯å†å²")
                    continue
                elif user_input.lower().startswith('save '):
                    filename = user_input[5:].strip()
                    if filename:
                        self.save_conversation(filename)
                    else:
                        print("âŒ è¯·æŒ‡å®šæ–‡ä»¶å")
                    continue
                
                # æ­£å¸¸å¯¹è¯
                print("ğŸ¤– AIæ­£åœ¨æ€è€ƒ...")
                response = self.chat(user_input)
                print(f"ğŸ¤– AI: {response}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def update_config(self, **kwargs):
        """
        æ›´æ–°é…ç½®å‚æ•°
        
        Args:
            **kwargs: è¦æ›´æ–°çš„å‚æ•°
        """
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)
                # å¦‚æœæ›´æ–°äº†APIç›¸å…³é…ç½®ï¼Œéœ€è¦æ›´æ–°headers
                if key == "api_key":
                    self.headers["Authorization"] = f"Bearer {self.api_key}"
            else:
                print(f"è­¦å‘Š: æœªçŸ¥å‚æ•° {key}")
    
    def get_config(self):
        """
        è·å–å½“å‰é…ç½®
        
        Returns:
            dict: å½“å‰é…ç½®å‚æ•°
        """
        return {
            "api_url": self.api_url,
            "model": self.model,
            "stream": self.stream,
            "max_tokens": self.max_tokens,
            "enable_thinking": self.enable_thinking,
            "thinking_budget": self.thinking_budget,
            "min_p": self.min_p,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "frequency_penalty": self.frequency_penalty,
            "presence_penalty": self.presence_penalty,
            "n": self.n,
            "stop": self.stop,
            "max_history_length": self.max_history_length,
            "auto_truncate": self.auto_truncate,
            "history_length": len(self.conversation_history)
        }


# ===== Demo ç¤ºä¾‹ =====
if __name__ == "__main__":
    print("ğŸš€ LLMClient Demo å¼€å§‹")
    print("="*50)
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹
    llm = LLMClient(
        api_key="sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx",  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
        model="THUDM/GLM-4-9B-0414",
        temperature=0.7,  # ç¨å¾®æé«˜éšæœºæ€§
        max_tokens=1024,   # é™ä½æœ€å¤§tokenæ•°ç”¨äºdemo
        max_history_length=10  # é™åˆ¶å†å²é•¿åº¦ç”¨äºdemo
    )
    
    # ç¤ºä¾‹1ï¼šå•æ¬¡è°ƒç”¨ï¼ˆä¸ä¿å­˜å†å²ï¼‰
    print("\nğŸ“ ç¤ºä¾‹1ï¼šå•æ¬¡è°ƒç”¨ï¼ˆforwardæ–¹æ³•ï¼‰")
    try:
        response = llm.forward("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
        print(f"ğŸ¤– å›å¤: {response}")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    
    # ç¤ºä¾‹2ï¼šè®¾ç½®ç³»ç»Ÿæç¤ºè¯å¹¶å¼€å§‹å¤šè½®å¯¹è¯
    print("\nğŸ“ ç¤ºä¾‹2ï¼šå¤šè½®å¯¹è¯åŠŸèƒ½")
    llm.set_system_prompt("ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—¥å¿—åˆ†æåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚")
    
    try:
        # ç¬¬ä¸€è½®å¯¹è¯
        print("ğŸ‘¤ ç”¨æˆ·: ä»€ä¹ˆæ˜¯æ—¥å¿—å¼‚å¸¸æ£€æµ‹ï¼Ÿ")
        response1 = llm.chat("ä»€ä¹ˆæ˜¯æ—¥å¿—å¼‚å¸¸æ£€æµ‹ï¼Ÿ")
        print(f"ğŸ¤– AI: {response1}")
        
        # ç¬¬äºŒè½®å¯¹è¯ï¼ˆåŸºäºå†å²ä¸Šä¸‹æ–‡ï¼‰
        print("\nğŸ‘¤ ç”¨æˆ·: æœ‰å“ªäº›å¸¸è§çš„æ£€æµ‹æ–¹æ³•ï¼Ÿ")
        response2 = llm.chat("æœ‰å“ªäº›å¸¸è§çš„æ£€æµ‹æ–¹æ³•ï¼Ÿ")
        print(f"ğŸ¤– AI: {response2}")
        
        # ç¬¬ä¸‰è½®å¯¹è¯
        print("\nğŸ‘¤ ç”¨æˆ·: èƒ½ç»™ä¸ªå…·ä½“ä¾‹å­å—ï¼Ÿ")
        response3 = llm.chat("èƒ½ç»™ä¸ªå…·ä½“ä¾‹å­å—ï¼Ÿ")
        print(f"ğŸ¤– AI: {response3}")
        
    except Exception as e:
        print(f"âŒ å¤šè½®å¯¹è¯é”™è¯¯: {e}")
    
    # ç¤ºä¾‹3ï¼šæŸ¥çœ‹å¯¹è¯å†å²
    print("\nğŸ“ ç¤ºä¾‹3ï¼šæŸ¥çœ‹å¯¹è¯å†å²")
    print(f"ğŸ“Š å½“å‰å†å²é•¿åº¦: {llm.get_history_length()} æ¡æ¶ˆæ¯")
    
    history_text = llm.get_history("text")
    if history_text:
        print("ğŸ“‹ å¯¹è¯å†å²æ‘˜è¦:")
        print("-" * 30)
        # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦é¿å…è¾“å‡ºè¿‡é•¿
        print(history_text[:200] + "..." if len(history_text) > 200 else history_text)
    
    # ç¤ºä¾‹4ï¼šä¿å­˜å’ŒåŠ è½½å¯¹è¯
    print("\nğŸ“ ç¤ºä¾‹4ï¼šä¿å­˜å¯¹è¯åŠŸèƒ½")
    try:
        conversation_file = "demo_conversation.json"
        llm.save_conversation(conversation_file)
        print(f"ğŸ’¾ å¯¹è¯å·²ä¿å­˜åˆ° {conversation_file}")
        
        # åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯å®ä¾‹å¹¶åŠ è½½å¯¹è¯
        llm2 = LLMClient(
            api_key="sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx",
            model="THUDM/GLM-4-9B-0414"
        )
        llm2.load_conversation(conversation_file)
        print(f"ğŸ“‚ æ–°å®¢æˆ·ç«¯å·²åŠ è½½å¯¹è¯ï¼Œå†å²é•¿åº¦: {llm2.get_history_length()}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜/åŠ è½½é”™è¯¯: {e}")
    
    # ç¤ºä¾‹5ï¼šæ¸…ç©ºå†å²å¹¶é‡æ–°å¼€å§‹
    print("\nğŸ“ ç¤ºä¾‹5ï¼šæ¸…ç©ºå†å²")
    print(f"æ¸…ç©ºå‰å†å²é•¿åº¦: {llm.get_history_length()}")
    llm.clear_history()
    print(f"æ¸…ç©ºåå†å²é•¿åº¦: {llm.get_history_length()}")
    
    # ç¤ºä¾‹6ï¼šé…ç½®ç®¡ç†
    print("\nğŸ“ ç¤ºä¾‹6ï¼šé…ç½®ç®¡ç†")
    print("åŸå§‹é…ç½®ï¼ˆéƒ¨åˆ†ï¼‰:")
    config = llm.get_config()
    for key in ['temperature', 'max_tokens', 'max_history_length', 'history_length']:
        print(f"  {key}: {config[key]}")
    
    # æ›´æ–°é…ç½®
    llm.update_config(temperature=0.1, max_history_length=20)
    print("\næ›´æ–°åé…ç½®:")
    new_config = llm.get_config()
    for key in ['temperature', 'max_tokens', 'max_history_length', 'history_length']:
        print(f"  {key}: {new_config[key]}")
    
    # ç¤ºä¾‹7ï¼šäº¤äº’å¼èŠå¤©æ¼”ç¤ºï¼ˆæ³¨é‡Šæ‰ï¼Œé¿å…é˜»å¡demoï¼‰
    print("\nğŸ“ ç¤ºä¾‹7ï¼šäº¤äº’å¼èŠå¤©æ¨¡å¼")
    print("ğŸ’¡ è¦å¯åŠ¨äº¤äº’å¼èŠå¤©ï¼Œè¯·å–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šï¼š")
    print("# llm.start_interactive_chat()")
    
    print("\nğŸ‰ Demo ç»“æŸ")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("  ğŸ”¹ ä½¿ç”¨ forward() è¿›è¡Œå•æ¬¡è°ƒç”¨ï¼Œä¸ä¿å­˜å†å²")
    print("  ğŸ”¹ ä½¿ç”¨ chat() è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œè‡ªåŠ¨ä¿å­˜å†å²")
    print("  ğŸ”¹ ä½¿ç”¨ set_system_prompt() è®¾ç½®ç³»ç»Ÿè§’è‰²")
    print("  ğŸ”¹ ä½¿ç”¨ clear_history() æ¸…ç©ºå¯¹è¯å†å²")
    print("  ğŸ”¹ ä½¿ç”¨ save_conversation() å’Œ load_conversation() ç®¡ç†å¯¹è¯")
    print("  ğŸ”¹ ä½¿ç”¨ start_interactive_chat() å¯åŠ¨äº¤äº’å¼èŠå¤©") 