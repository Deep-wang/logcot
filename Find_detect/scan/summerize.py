import os
import requests
from tenacity import retry, stop_after_attempt, wait_exponential

API_URL = "https://api.siliconflow.cn/v1/chat/completions"
API_KEY = "sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx" # æ›¿æ¢ä¸ºä½ çš„å®é™… Key
MAX_CHARS_PER_CHUNK = 7000  # æ§åˆ¶æ¨¡å‹å•æ¬¡æœ€å¤§è¾“å…¥

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1.5, min=2, max=10))
def call_model(prompt):
    payload = {
        "model": "Qwen/Qwen3-8B",
        "stream": False,
        "max_tokens": 8192,
        "enable_thinking": True,
        "thinking_budget": 4096,
        "min_p": 0.05,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "stop": [],
        "messages": [{"role": "user", "content": prompt}],
    }

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    response = requests.post(API_URL, json=payload, headers=headers)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

# è¯»å–ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶å¹¶åˆå¹¶ä¸ºä¸€å¤§æ®µæ–‡æœ¬
def load_all_logs_to_string(root_dir):
    combined = ""
    for root, _, files in os.walk(root_dir):
        for fname in files:
            if fname.endswith(".txt"):
                path = os.path.join(root, fname)
                try:
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                        combined += f"\nã€æ–‡ä»¶ã€‘ï¼š{os.path.relpath(path, root_dir)}\n{content}\n"
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {path}: {e}")
    return combined

# å°†å¤§æ–‡æœ¬æŒ‰å­—æ•°åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ
def split_text_into_chunks(text, max_chars):
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]

# ä¸»é€»è¾‘ï¼šè¯»å–+åˆ†æ®µåˆ†æ+æœ€ç»ˆæ€»ç»“
def analyze_log_directory(root_dir):
    all_logs = load_all_logs_to_string(root_dir)
    print(f"ğŸ“„ æ—¥å¿—æ€»é•¿åº¦ï¼š{len(all_logs)} å­—ç¬¦")

    chunks = split_text_into_chunks(all_logs, MAX_CHARS_PER_CHUNK)
    print(f"ğŸ” åˆ†ä¸º {len(chunks)} æ®µè¿›è¡Œåˆ†æ")

    partial_results = []
    for i, chunk in enumerate(chunks):
        print(f"ğŸš€ æäº¤ç¬¬ {i + 1} æ®µåˆ†æ...")
        prompt = (
            f"æ—¥å¿—åˆ†æç¬¬ {i + 1} éƒ¨åˆ†ï¼š\n\n{chunk}\n\n"
            "è¯·è¯†åˆ«å…¶ä¸­æ˜¯å¦æœ‰å¼‚å¸¸æƒ…å†µã€æ•…éšœæ—¶é—´ç‚¹ã€ä»¥åŠæ—¥å¿—ç±»å‹ä¹‹é—´å¯èƒ½çš„å› æœå…³ç³»ã€‚æ€»ç»“å¼‚å¸¸ç‚¹å¹¶æå–å…³é”®è¯´æ˜ã€‚"
        )
        result = call_model(prompt)
        partial_results.append(result)

    print("ğŸ§  å‡†å¤‡æäº¤æ•´åˆåˆ†æ...")
    full_summary_input = "\n\n".join(
        f"ç¬¬{i+1}æ®µåˆ†æç»“æœï¼š\n{res}" for i, res in enumerate(partial_results)
    )
    final_prompt = (
        f"ä»¥ä¸‹æ˜¯å¤šä¸ªæ—¥å¿—åˆ†æéƒ¨åˆ†çš„ç»“æœï¼Œè¯·ç»¼åˆè¿™äº›ä¿¡æ¯å›ç­”ï¼š\n"
        f"1. æ˜¯å¦å­˜åœ¨å…±æ€§æˆ–é‡å¤çš„é—®é¢˜ï¼Ÿ\n"
        f"2. æ˜¯å¦å¯ä»¥åˆ¤æ–­æ•…éšœåŸå› æˆ–å½±å“èŒƒå›´ï¼Ÿ\n"
        f"3. å„æ—¥å¿—ç±»å‹ä¹‹é—´æ˜¯å¦å­˜åœ¨ä¾èµ–æˆ–å› æœè”ç³»ï¼Ÿ\n\n"
        f"{full_summary_input}"
    )

    final_result = call_model(final_prompt)
    print("\nâœ… æœ€ç»ˆæ•´åˆæ€»ç»“ï¼š\n")
    print(final_result)
    

# ç¤ºä¾‹ä½¿ç”¨
# if __name__ == "__main__":
#     analyze_log_directory('/Users/hy_mbp/output3')  # å°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºä½ çš„å®é™…æ—¥å¿—æ–‡ä»¶ç›®å½•
