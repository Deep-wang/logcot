"""
æ•´åˆåˆ†æ
è¾“å…¥ï¼šoutput_529æ–‡ä»¶å¤¹
è¾“å‡ºï¼šæ•´åˆä¸€ä¸ªæ€»ç»“ï¼Œåˆ†æé”™è¯¯æ—¥å¿—
"""

import os
import sys
import requests
from tenacity import retry, stop_after_attempt, wait_exponential
# å¯¼å…¥LLMClientç±»

import re

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥LLMClient
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from llm_client import LLMClient

API_URL = "https://api.siliconflow.cn/v1/chat/completions"
API_KEY = "sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx" # æ›¿æ¢ä¸ºä½ çš„å®é™… Key
MAX_CHARS_PER_CHUNK = 7000  # æ§åˆ¶æ¨¡å‹å•æ¬¡æœ€å¤§è¾“å…¥

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1.5, min=2, max=10))
def call_model(prompt):
    payload = {
        "model": "Qwen/Qwen3-8B",
        "stream": False,
        "max_tokens": 8192,
        "enable_thinking": True,
        "thinking_budget": 4096,
        "min_p": 0.05,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "stop": [],
        "messages": [{"role": "user", "content": prompt}],
    }

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    response = requests.post(API_URL, json=payload, headers=headers)
    response.raise_for_status()
    return response.json()["choices"][0]["message"]["content"]

def create_critic_model(model_name="THUDM/GLM-4-9B-0414"):
    """
    åˆ›å»ºè¯„åˆ¤æ¨¡å‹ï¼Œç”¨äºè¯„ä¼°åˆ†æç»“æœçš„è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®
    
    Returns:
        LLMClient: é…ç½®å¥½çš„è¯„åˆ¤æ¨¡å‹å®ä¾‹
    """
    critic_llm = LLMClient(
        api_key=API_KEY,
        api_url=API_URL,
        model=model_name,  # ä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹ä½œä¸ºè¯„åˆ¤è€…
        temperature=0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿è¯„åˆ¤çš„ä¸€è‡´æ€§
        max_tokens=4096,
        enable_thinking=True,
        thinking_budget=2048,
        top_p=0.8,
        frequency_penalty=0.2
    )
    return critic_llm

def critic_analysis(analysis_result, original_logs_sample="", model_name="THUDM/GLM-4-9B-0414"):
    """
    ä½¿ç”¨è¯„åˆ¤æ¨¡å‹å¯¹åˆ†æç»“æœè¿›è¡Œè¯„ä¼°å’Œæ”¹è¿›å»ºè®®
    
    Args:
        analysis_result: åŸå§‹åˆ†æç»“æœ
        original_logs_sample: åŸå§‹æ—¥å¿—æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: è¯„åˆ¤ç»“æœå’Œæ”¹è¿›å»ºè®®
    """
    # Meta Prompt - èµ‹äºˆå¤§æ¨¡å‹è¯„åˆ¤è€…èº«ä»½
    meta_prompt = """
    ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ—¥å¿—åˆ†æä¸“å®¶å’Œè´¨é‡è¯„ä¼°å¸ˆï¼Œå…·æœ‰ä»¥ä¸‹ä¸“ä¸šèƒ½åŠ›ï¼š
    
    ã€èº«ä»½å®šä½ã€‘
    - æ‹¥æœ‰10å¹´ä»¥ä¸Šçš„ç³»ç»Ÿè¿ç»´å’Œæ—¥å¿—åˆ†æç»éªŒ
    - ç†Ÿæ‚‰å„ç§ç±»å‹çš„ç³»ç»Ÿæ—¥å¿—ã€åº”ç”¨æ—¥å¿—ã€é”™è¯¯æ—¥å¿—
    - æ“…é•¿è¯†åˆ«æ—¥å¿—åˆ†æä¸­çš„ç›²ç‚¹ã€è¯¯åˆ¤å’Œé—æ¼
    - å…·å¤‡æ•é”çš„è´¨é‡è¯„ä¼°èƒ½åŠ›å’Œæ”¹è¿›æ–¹æ¡ˆè®¾è®¡èƒ½åŠ›
    
    ã€è¯„ä¼°ç»´åº¦ã€‘
    è¯·ä»ä»¥ä¸‹ç»´åº¦å¯¹æ—¥å¿—åˆ†æç»“æœè¿›è¡Œä¸“ä¸šè¯„ä¼°ï¼š
    
    1. **å‡†ç¡®æ€§è¯„ä¼°**ï¼š
       - å¼‚å¸¸è¯†åˆ«æ˜¯å¦å‡†ç¡®ï¼Ÿæ˜¯å¦å­˜åœ¨è¯¯æŠ¥æˆ–æ¼æŠ¥ï¼Ÿ
       - æ—¶é—´ç‚¹å®šä½æ˜¯å¦ç²¾ç¡®ï¼Ÿ
       - å› æœå…³ç³»åˆ†ææ˜¯å¦åˆç†ï¼Ÿ
    
    2. **å®Œæ•´æ€§è¯„ä¼°**ï¼š
       - æ˜¯å¦é—æ¼äº†é‡è¦çš„å¼‚å¸¸ä¿¡æ¯ï¼Ÿ
       - åˆ†ææ·±åº¦æ˜¯å¦è¶³å¤Ÿï¼Ÿ
       - å…³é”®ç»†èŠ‚æ˜¯å¦è¢«å¿½è§†ï¼Ÿ
    
    3. **é€»è¾‘æ€§è¯„ä¼°**ï¼š
       - åˆ†æé€»è¾‘æ˜¯å¦æ¸…æ™°åˆç†ï¼Ÿ
       - ç»“è®ºæ˜¯å¦ä¸è¯æ®åŒ¹é…ï¼Ÿ
       - æ¨ç†è¿‡ç¨‹æ˜¯å¦å­˜åœ¨è·³è·ƒæˆ–çŸ›ç›¾ï¼Ÿ
    
    4. **å®ç”¨æ€§è¯„ä¼°**ï¼š
       - åˆ†æç»“æœå¯¹é—®é¢˜è§£å†³çš„å¸®åŠ©ç¨‹åº¦
       - æ˜¯å¦æä¾›äº†å…·ä½“å¯è¡Œçš„æ’æŸ¥æ–¹å‘ï¼Ÿ
       - ä¼˜å…ˆçº§åˆ¤æ–­æ˜¯å¦åˆç†ï¼Ÿ
    
    ã€è¾“å‡ºè¦æ±‚ã€‘
    è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æä¾›è¯„ä¼°ç»“æœï¼š
    
    ## ğŸ¯ ç»¼åˆè¯„åˆ†
    [ç»™å‡º1-10åˆ†çš„è¯„åˆ†ï¼Œå¹¶ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±, æ³¨æ„è¯„åˆ†éœ€è¦æŒ‰ç…§ä¸‹é¢æ ¼å¼ç»™å‡º(ä¾‹å­ï¼šè¯„åˆ†ï¼š5åˆ†)]
    
    ## âœ… åˆ†æä¼˜ç‚¹
    [åˆ—å‡ºå½“å‰åˆ†æçš„äº®ç‚¹å’Œæ­£ç¡®ä¹‹å¤„]
    
    ## âš ï¸ å­˜åœ¨é—®é¢˜
    [æŒ‡å‡ºåˆ†æä¸­çš„ä¸è¶³ã€é”™è¯¯æˆ–é—æ¼]
    
    ## ğŸš€ æ”¹è¿›å»ºè®®
    [æä¾›å…·ä½“çš„æ”¹è¿›æ–¹å‘å’Œä¼˜åŒ–å»ºè®®]
    
    ## ğŸ“‹ è¡¥å……åˆ†æ
    [å¦‚æœå‘ç°é—æ¼çš„é‡è¦ä¿¡æ¯ï¼Œè¯·è¡¥å……åˆ†æ]
    
    è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šã€å»ºè®¾æ€§çš„è¯„ä¼°æ€åº¦ã€‚
    """
    
    # æ„å»ºè¯„åˆ¤è¾“å…¥å†…å®¹
    content = f"""
    è¯·å¯¹ä»¥ä¸‹æ—¥å¿—åˆ†æç»“æœè¿›è¡Œä¸“ä¸šè¯„ä¼°ï¼š
    
    ## å¾…è¯„ä¼°çš„åˆ†æç»“æœï¼š
    {analysis_result}
    
    ## åŸå§‹æ—¥å¿—æ ·æœ¬ï¼ˆå‚è€ƒï¼‰ï¼š
    {original_logs_sample if original_logs_sample else "æš‚æ— åŸå§‹æ—¥å¿—æ ·æœ¬"}
    
    è¯·æ ¹æ®ä½ çš„ä¸“ä¸šç»éªŒï¼Œå¯¹ä¸Šè¿°åˆ†æç»“æœè¿›è¡Œå…¨é¢è¯„ä¼°å¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚
    """
    
    # åˆ›å»ºè¯„åˆ¤æ¨¡å‹
    critic = create_critic_model(model_name=model_name)
        
    try:
        # è°ƒç”¨è¯„åˆ¤æ¨¡å‹
        critic_result = critic.forward(content, system_prompt=meta_prompt)
        score = extract_score_from_critic_result(critic_result) 
        return critic_result, score
    except Exception as e:
        return f"âŒ è¯„åˆ¤æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}", None

# è¯»å–ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶å¹¶åˆå¹¶ä¸ºä¸€å¤§æ®µæ–‡æœ¬
def load_all_logs_to_string(root_dir):
    combined = ""
    for root, _, files in os.walk(root_dir):
        for fname in files:
            if fname.endswith(".txt"):
                path = os.path.join(root, fname)
                try:
                    with open(path, "r", encoding="utf-8", errors="ignore") as f:
                        content = f.read()
                        combined += f"\nã€æ–‡ä»¶ã€‘ï¼š{os.path.relpath(path, root_dir)}\n{content}\n"
                except Exception as e:
                    print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {path}: {e}")
    return combined


def extract_score_from_critic_result(critic_result):
    """
    ä»è¯„åˆ¤æ¨¡å‹ç»“æœä¸­æå–è¯„åˆ†
    
    Args:
        critic_result: è¯„åˆ¤æ¨¡å‹çš„è¾“å‡ºç»“æœ
        
    Returns:
        float: æå–çš„è¯„åˆ†ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›None
    """
    if not critic_result:
        return None
        
    # å¤šç§æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åŒ¹é…è¯„åˆ†
    score_patterns = [
        r'ç»¼åˆè¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',  # ç»¼åˆè¯„åˆ†ï¼š8.5
        r'è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',      # è¯„åˆ†ï¼š8
        r'(\d+(?:\.\d+)?)\s*åˆ†',             # 8.5åˆ†
        r'(\d+(?:\.\d+)?)/10',               # 8/10
        r'å¾—åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',      # å¾—åˆ†ï¼š8.5
        r'åˆ†æ•°[ï¼š:]\s*(\d+(?:\.\d+)?)',      # åˆ†æ•°ï¼š8.5
    ]
    
    for pattern in score_patterns:
        match = re.search(pattern, critic_result, re.IGNORECASE)
        if match:
            try:
                score = float(match.group(1))
                # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
                if 0 <= score <= 10:
                    return score
            except ValueError:
                continue
                
    return None


# å°†å¤§æ–‡æœ¬æŒ‰å­—æ•°åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ
def split_text_into_chunks(text, max_chars):
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]

def iterative_improve_analysis(initial_result, logs_sample, max_iterations=3, target_score=8.0, model_name="Qwen/Qwen3-8B"):
    """
    åŸºäºè¯„åˆ¤ç»“æœçš„è¿­ä»£ä¼˜åŒ–åˆ†æ
    
    Args:
        initial_result: åˆå§‹åˆ†æç»“æœ
        logs_sample: åŸå§‹æ—¥å¿—æ ·æœ¬
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        target_score: ç›®æ ‡è¯„åˆ†é˜ˆå€¼
        model_name: ä½¿ç”¨çš„æ¨¡å‹åç§°
        
    Returns:
        tuple: (æœ€ç»ˆä¼˜åŒ–ç»“æœ, æœ€ç»ˆè¯„åˆ†, è¿­ä»£å†å²)
    """
    print(f"\nğŸ”„ å¼€å§‹è¿­ä»£ä¼˜åŒ–åˆ†æï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}, ç›®æ ‡è¯„åˆ†: {target_score}ï¼‰")
    print("=" * 60)
    
    # åˆ›å»ºç”¨äºä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯
    optimization_llm = LLMClient(
        api_url=API_URL,
        api_key=API_KEY,
        model=model_name,
        temperature=0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿ä¼˜åŒ–çš„ä¸€è‡´æ€§
        max_tokens=8192,
        enable_thinking=True,
        thinking_budget=4096,
        max_history_length=20,  # ä¿ç•™è¶³å¤Ÿçš„ä¼˜åŒ–å†å²
        auto_truncate=True
    )
    
    # è®¾ç½®ç³»ç»Ÿæç¤ºè¯
    system_prompt = """
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—¥å¿—åˆ†æä¸“å®¶ï¼Œæ­£åœ¨ä¼˜åŒ–å’Œæ”¹è¿›è‡ªå·±çš„åˆ†æç»“æœã€‚

    ä½ çš„ä»»åŠ¡æ˜¯ï¼š
    1. ä»”ç»†é˜…è¯»è¯„åˆ¤ä¸“å®¶æä¾›çš„è¯„ä¼°æŠ¥å‘Šå’Œæ”¹è¿›å»ºè®®
    2. æ ¹æ®è¯„åˆ¤ç»“æœä¸­æŒ‡å‡ºçš„é—®é¢˜å’Œä¸è¶³ï¼Œæ”¹è¿›ä½ çš„åˆ†æ
    3. æä¾›æ›´å‡†ç¡®ã€æ›´å®Œæ•´ã€æ›´æœ‰ç”¨çš„æ—¥å¿—åˆ†æç»“æœ
    4. ç¡®ä¿æ”¹è¿›åçš„åˆ†æé€»è¾‘æ¸…æ™°ã€è¯æ®å……åˆ†ã€ç»“è®ºåˆç†

    è¯·ä¿æŒåˆ†æçš„ä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æ ¹æ®è¯„åˆ¤å»ºè®®è¿›è¡Œé’ˆå¯¹æ€§æ”¹è¿›ã€‚
    """
    
    optimization_llm.set_system_prompt(system_prompt)
    
    current_result = initial_result
    iteration_history = []
    
    for iteration in range(max_iterations):
        print(f"\nğŸ” === ç¬¬ {iteration + 1} è½®ä¼˜åŒ– ===")
        
        # è·å–å½“å‰ç»“æœçš„è¯„åˆ¤
        critic_result, score = critic_analysis(current_result, logs_sample, model_name=model_name)
        
        print(f"ğŸ“Š å½“å‰è¯„åˆ†: {score}")
        
        # è®°å½•æœ¬è½®å†å²
        iteration_info = {
            "iteration": iteration + 1,
            "result": current_result,
            "score": score,
            "critic_result": critic_result
        }
        iteration_history.append(iteration_info)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡è¯„åˆ†
        if score is not None and score >= target_score:
            print(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡è¯„åˆ† {target_score}ï¼Œä¼˜åŒ–å®Œæˆï¼")
            break
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€è½®
        if iteration == max_iterations - 1:
            print(f"ğŸ“‹ å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œä¼˜åŒ–ç»“æŸ")
            break
        
        # æ„å»ºä¼˜åŒ–æç¤º
        improvement_prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹è¯„åˆ¤ç»“æœæ”¹è¿›ä½ çš„æ—¥å¿—åˆ†æï¼š

        ## å½“å‰åˆ†æç»“æœï¼š
        {current_result}

        ## è¯„åˆ¤ä¸“å®¶çš„è¯„ä¼°æŠ¥å‘Šï¼š
        {critic_result}

        ## åŸå§‹æ—¥å¿—æ ·æœ¬ï¼ˆå‚è€ƒï¼‰ï¼š
        {logs_sample[:1500] if logs_sample else "æ— æ—¥å¿—æ ·æœ¬"}

        è¯·æ ¹æ®è¯„åˆ¤ä¸“å®¶çš„å»ºè®®ï¼Œæ”¹è¿›ä¸Šè¿°åˆ†æç»“æœã€‚ç‰¹åˆ«å…³æ³¨ï¼š
        1. è¯„åˆ¤ä¸­æŒ‡å‡ºçš„å‡†ç¡®æ€§é—®é¢˜
        2. é—æ¼çš„é‡è¦ä¿¡æ¯
        3. é€»è¾‘æ€§å’Œå®Œæ•´æ€§æ–¹é¢çš„ä¸è¶³
        4. å®ç”¨æ€§å’Œå¯æ“ä½œæ€§çš„æ”¹è¿›ç©ºé—´

        è¯·æä¾›æ”¹è¿›åçš„å®Œæ•´åˆ†æç»“æœï¼š
        """
        
        try:
            print("ğŸ¤– AIæ­£åœ¨åŸºäºè¯„åˆ¤ç»“æœè¿›è¡Œä¼˜åŒ–...")
            improved_result = optimization_llm.chat(improvement_prompt)
            current_result = improved_result
            print(f"âœ… ç¬¬ {iteration + 1} è½®ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬ {iteration + 1} è½®ä¼˜åŒ–å¤±è´¥: {e}")
            break
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\nğŸ === ä¼˜åŒ–è¿‡ç¨‹å®Œæˆ ===")
    final_critic_result, final_score = critic_analysis(current_result, logs_sample, model_name=model_name)
    
    # è¾“å‡ºä¼˜åŒ–æ€»ç»“
    print(f"\nğŸ“ˆ ä¼˜åŒ–æ€»ç»“:")
    print(f"  ğŸ”¢ æ€»è¿­ä»£æ¬¡æ•°: {len(iteration_history)}")
    print(f"  ğŸ“Š åˆå§‹è¯„åˆ†: {iteration_history[0]['score'] if iteration_history else 'N/A'}")
    print(f"  ğŸ“Š æœ€ç»ˆè¯„åˆ†: {final_score}")
    
    if len(iteration_history) > 0 and iteration_history[0]['score'] is not None and final_score is not None:
        improvement = final_score - iteration_history[0]['score']
        print(f"  ğŸ“ˆ è¯„åˆ†æå‡: {improvement:+.1f}")
        if improvement > 0:
            print(f"  âœ… åˆ†æè´¨é‡å¾—åˆ°æ”¹å–„")
        elif improvement == 0:
            print(f"  â– åˆ†æè´¨é‡ä¿æŒç¨³å®š")
        else:
            print(f"  âš ï¸ è¯„åˆ†æœ‰æ‰€ä¸‹é™")
    
    return current_result, final_score, iteration_history

# ä¸»é€»è¾‘ï¼šè¯»å–+åˆ†æ®µåˆ†æ+æœ€ç»ˆæ€»ç»“
def analyze_log_directory(root_dir, option='dir'):
    if option == 'dir':
        all_logs = load_all_logs_to_string(root_dir)
        print(f"ğŸ“„ æ—¥å¿—æ€»é•¿åº¦ï¼š{len(all_logs)} å­—ç¬¦")
        # ä¿å­˜æ—¥å¿—æ ·æœ¬ç”¨äºè¯„åˆ¤æ¨¡å‹å‚è€ƒ
        logs_sample = all_logs[:2000] if len(all_logs) > 2000 else all_logs
    elif option == 'str':
        all_logs = root_dir
        logs_sample = all_logs[:2000] if len(all_logs) > 2000 else all_logs

    chunks = split_text_into_chunks(all_logs, MAX_CHARS_PER_CHUNK)
    print(f"ğŸ” åˆ†ä¸º {len(chunks)} æ®µè¿›è¡Œåˆ†æ")

    partial_results = []
    for i, chunk in enumerate(chunks):
        print(f"ğŸš€ æäº¤ç¬¬ {i + 1} æ®µåˆ†æ...")
        prompt = (
            f"æ—¥å¿—åˆ†æç¬¬ {i + 1} éƒ¨åˆ†ï¼š\n\n{chunk}\n\n"
            "è¯·è¯†åˆ«å…¶ä¸­æ˜¯å¦æœ‰å¼‚å¸¸æƒ…å†µã€æ•…éšœæ—¶é—´ç‚¹ã€ä»¥åŠæ—¥å¿—ç±»å‹ä¹‹é—´å¯èƒ½çš„å› æœå…³ç³»ã€‚æ€»ç»“å¼‚å¸¸ç‚¹å¹¶æå–å…³é”®è¯´æ˜ã€‚"
        )
        result = call_model(prompt)
        partial_results.append(result)

    print("ğŸ§  å‡†å¤‡æäº¤æ•´åˆåˆ†æ...")
    full_summary_input = "\n\n".join(
        f"ç¬¬{i+1}æ®µåˆ†æç»“æœï¼š\n{res}" for i, res in enumerate(partial_results)
    )
    final_prompt = (
        f"ä»¥ä¸‹æ˜¯å¤šä¸ªæ—¥å¿—åˆ†æéƒ¨åˆ†çš„ç»“æœï¼Œè¯·ç»¼åˆè¿™äº›ä¿¡æ¯å›ç­”ï¼š\n"
        f"1. æ˜¯å¦å­˜åœ¨å…±æ€§æˆ–é‡å¤çš„é—®é¢˜ï¼Ÿ\n"
        f"2. æ˜¯å¦å¯ä»¥åˆ¤æ–­æ•…éšœåŸå› æˆ–å½±å“èŒƒå›´ï¼Ÿ\n"
        f"3. å„æ—¥å¿—ç±»å‹ä¹‹é—´æ˜¯å¦å­˜åœ¨ä¾èµ–æˆ–å› æœè”ç³»ï¼Ÿ\n\n"
        f"{full_summary_input}"
    )

    # final_result = call_model(final_prompt)
    llm = LLMClient(
        api_url=API_URL,
        api_key=API_KEY,
        model="Qwen/Qwen3-8B",
        stream=False,
        max_tokens=8192,
        enable_thinking=True,
        thinking_budget=4096,
        min_p=0.1,
        temperature=0.1,
        top_p=0.3,
        top_k=20,
        frequency_penalty=0.1,
        presence_penalty=0.0,
        n=1,
        stop=[]
    )
    initial_result = llm.forward(final_prompt)

    print("\nâœ… åˆå§‹åˆ†æç»“æœï¼š\n")
    print(initial_result)
    
    # ä½¿ç”¨è¿­ä»£ä¼˜åŒ–åŠŸèƒ½æ”¹è¿›åˆ†æç»“æœ
    final_result, final_score, optimization_history = iterative_improve_analysis(
        initial_result=initial_result,
        logs_sample=logs_sample,
        max_iterations=3,  # æœ€å¤š3è½®ä¼˜åŒ–
        target_score=8.0,  # ç›®æ ‡è¯„åˆ†8åˆ†
        model_name="Qwen/Qwen3-8B"
    )
    
    print(f"\nğŸ¯ æœ€ç»ˆä¼˜åŒ–è¯„åˆ†ï¼š{final_score}")
    print(f"\nâœ¨ === æœ€ç»ˆä¼˜åŒ–åçš„åˆ†æç»“æœ ===")
    print(final_result)
    
    return final_result

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'output_529')
    analyze_log_directory(path)  # å°†æ­¤è·¯å¾„æ›¿æ¢ä¸ºä½ çš„å®é™…æ—¥å¿—æ–‡ä»¶ç›®å½•
