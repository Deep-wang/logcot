"""
åˆ†å—åˆ†æ
è¾“å‡ºoutput529,é‡Œé¢åŒ…å«è¯Šæ–­ä¸ºé”™è¯¯æ—¥å¿—çš„å—ï¼Œtxtæ–‡ä»¶
ä¸»å‡½æ•°åšæ—¶é—´åˆ†ç»„ï¼Œç„¶åè°ƒç”¨File_Då‡½æ•°ï¼Œç„¶åè°ƒç”¨analyze_log_directoryå‡½æ•°ï¼Œç„¶åä¿å­˜åˆ†æç»“æœåˆ°output529/analysis_resultç›®å½•ä¸‹
"""

import os
import sys
import requests
import json
from concurrent.futures import ThreadPoolExecutor
from tenacity import retry, stop_after_attempt, wait_exponential
from pathlib import Path

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥LLMClientå’Œå…¶ä»–æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from summerize import analyze_log_directory
from llm_client import LLMClient

# å¯¼å…¥æ—¥å¿—é¢„å¤„ç†æ¨¡å—
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from log_preprocessor import LogTimeGrouper
import time

def File_D(file_name,api_url,OUTPUT_DIR):
    current_dir = Path.cwd()
    LOG_RELATIVE_PATH = file_name 
    LOG_FILE_PATH = os.path.join(current_dir, LOG_RELATIVE_PATH)
    LINES_PER_CHUNK = 130 * 2
    MAX_WORKERS = 5
    API_KEYS = [
        "sk-dpadryupxccpbkigoduasfosszucawczlmfraqhtevaxlokx",
        "sk-kbucebwhrsoimqttlosgtxncyvmuvdyioncbadavayiovrns",
        "sk-zzuykfebwbxkurfftzuvujqpwqzxxljegnxhhwtzddvwiigf",
        "sk-qgsqryixuqdmtzkgubxpvdzollysgtonnvcrwmikwegmaogn",
        "sk-hvxqvahoplbhdadwtaomisdamxqhquvummcfpvlafeovpqus",
    ]

    log_path_sanitized = LOG_RELATIVE_PATH.replace("\\", '_')
    ERROR_DIR = os.path.join(OUTPUT_DIR, log_path_sanitized + "_errors")
    print(ERROR_DIR)
    os.makedirs(ERROR_DIR, exist_ok=True)
    all_analysis = Scannor(LOG_FILE_PATH, LINES_PER_CHUNK, MAX_WORKERS, API_KEYS, ERROR_DIR, api_url)
    return all_analysis


@retry(stop=stop_after_attempt(7), wait=wait_exponential(multiplier=2, min=2, max=100))
def post_with_retry(payload, headers, API_URL):
    response = requests.post(API_URL, json=payload, headers=headers)
    response.raise_for_status()
    return response.json()

def analyze_log_chunk(chunk, idx, api_key, error_dir, API_URL):
    # åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹
    llm = LLMClient(
        api_url=API_URL,
        api_key=api_key,
        model="THUDM/GLM-4-9B-0414",
        stream=False,
        max_tokens=8192,
        enable_thinking=True,
        thinking_budget=4096,
        min_p=0.1,
        temperature=0.1,
        top_p=0.3,
        top_k=20,
        frequency_penalty=0.1,
        presence_penalty=0.0,
        n=1,
        stop=[]
    )

    try:
        # ä½¿ç”¨LLMå®¢æˆ·ç«¯è¿›è¡Œè°ƒç”¨
        content = f"æ—¥å¿—ç¬¬ {idx + 1} éƒ¨åˆ†ï¼š\n{chunk}\n\nè¯·æ˜ç¡®æ ‡æ³¨è¯¥éƒ¨åˆ†æ˜¯å¦æ­£ç¡®ï¼Œæ­£ç¡®å†™ä¸ºï¼šæ—¥å¿—æ­£ç¡®ï¼Œå¼‚å¸¸å†™ä¸ºï¼šæ—¥å¿—å¼‚å¸¸ï¼Œæ ‡æ˜é”™è¯¯æ—¶é—´å¹¶æˆªå–å‡ºç°å¼‚å¸¸ç‚¹é™„è¿‘15è¡Œæ—¥å¿—å¹¶ç»™å‡ºç®€å•è¯´æ˜"
        result = llm.forward(content)
        print(f"âœ… ç¬¬ {idx + 1} å—æ—¥å¿—åˆ†æå®Œæˆ")

        if "æ—¥å¿—å¼‚å¸¸" in result:
            error_path = os.path.join(error_dir, f"error_block_{idx + 1}.txt")
            with open(error_path, "w", encoding="utf-8") as f:
                f.write("åˆ†æç»“æœï¼š\n")
                f.write(result)
            print(f"âš ï¸ å‘ç°å¼‚å¸¸ï¼Œå·²å†™å…¥ï¼š{error_path}")
        return f"ç¬¬ {idx + 1} éƒ¨åˆ†åˆ†æç»“æœï¼š\n{result}\n"

    except Exception as e:
        print(f"âŒ ç¬¬ {idx + 1} å—æ—¥å¿—åˆ†æå¤±è´¥ï¼š{e}")
        return f"ç¬¬ {idx + 1} éƒ¨åˆ†åˆ†æç»“æœï¼š\nåˆ†æå¤±è´¥ï¼š{e}\n"

def read_file_in_line_chunks(path, lines_per_chunk):
    with open(path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    return ["".join(lines[i:i + lines_per_chunk]) for i in range(0, len(lines), lines_per_chunk)]

def summarize_all_chunks(all_analysis, api_key, api_url):
    summary_prompt = "\n\n".join(all_analysis)
    payload = {
        "model": "Qwen/Qwen3-8B",
        "stream": False,
        "max_tokens": 8192,  # debug
        "enable_thinking": True,
        "thinking_budget": 4096,
        "min_p": 0.05,
        "temperature": 0,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "stop": [],
        "messages": [
            {
                "role": "user",
                "content": summary_prompt
            }
        ],
    }
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        response = post_with_retry(payload, headers, api_url)
        output = response['choices'][0]['message']['content']
        print("\nğŸ§¾ æœ€ç»ˆæ±‡æ€»åˆ†æç»“æœï¼š\n")
        print(output)
    except Exception as e:
        print(f"âš ï¸ æ±‡æ€»åˆ†æå¤±è´¥ï¼š{e}")
    return output

def Scannor(log_file_path, lines_per_chunk, max_workers, api_keys, error_dir, api_url):
    chunks = read_file_in_line_chunks(log_file_path, lines_per_chunk)
    print(f"å…±è¯»å– {len(chunks)} ä¸ªæ—¥å¿—å—ï¼Œå¼€å§‹å¹¶å‘åˆ†æ...\n")

    all_analysis = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for idx, chunk in enumerate(chunks):
            api_key = api_keys[idx % len(api_keys)]
            futures.append(executor.submit(analyze_log_chunk, chunk, idx, api_key, error_dir, api_url))

        for future in futures:
            result = future.result()
            all_analysis.append(result)

    # output = summarize_all_chunks(all_analysis, api_keys[0],api_url)
    return all_analysis

def UpLoad_File(dir_path):
    file_ls = []
    for root, dirs, files in os.walk(dir_path):
        root_file_ls = [os.path.join(root, file) for file in files]
        for file in root_file_ls:
            file_ls.append(file)
    # è¿‡æ»¤ .DS_Store æ–‡ä»¶
    file_ls = [file for file in file_ls if not file.endswith('.DS_Store')]
    return file_ls


def parse_time_range_from_filename(filename):
    """
    ä»æ–‡ä»¶åä¸­è§£ææ—¶é—´èŒƒå›´ä¿¡æ¯
    
    Args:
        filename: æ–‡ä»¶åï¼Œæ ¼å¼å¦‚ "czp_db1_auth_2025-04-23_1400-1600.log"
        
    Returns:
        tuple: (æ—¶é—´èŒƒå›´å­—ç¬¦ä¸², å¼€å§‹æ—¶é—´datetimeå¯¹è±¡) æˆ– (None, None)
    """
    import re
    from datetime import datetime
    
    # åŒ¹é…æ—¶é—´èŒƒå›´æ ¼å¼ï¼šYYYY-MM-DD_HHMM-HHMM
    pattern = r'(\d{4}-\d{2}-\d{2}_\d{4}-\d{4})'
    match = re.search(pattern, filename)
    
    if match:
        time_range_str = match.group(1)
        try:
            # è§£æå¼€å§‹æ—¶é—´ç”¨äºæ’åº
            date_part, time_part = time_range_str.split('_')
            start_time_str = time_part.split('-')[0]
            
            # ç»„åˆå®Œæ•´çš„å¼€å§‹æ—¶é—´
            start_datetime_str = f"{date_part} {start_time_str[:2]}:{start_time_str[2:]}"
            start_datetime = datetime.strptime(start_datetime_str, '%Y-%m-%d %H:%M')
            
            return time_range_str, start_datetime
        except Exception as e:
            print(f"  âš ï¸ è§£ææ—¶é—´å¤±è´¥ {filename}: {e}")
            return None, None
    
    return None, None


def group_files_by_time_range(file_list):
    """
    æŒ‰æ—¶é—´èŒƒå›´å¯¹æ–‡ä»¶è¿›è¡Œåˆ†ç»„
    
    Args:
        file_list: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        dict: {æ—¶é—´èŒƒå›´: [æ–‡ä»¶åˆ—è¡¨]}ï¼ŒæŒ‰æ—¶é—´æ’åº
    """
    from collections import defaultdict
    from datetime import datetime
    
    time_groups = defaultdict(list)
    time_order = {}  # ç”¨äºæ’åºçš„æ—¶é—´æ˜ å°„
    
    for file_path in file_list:
        filename = os.path.basename(file_path)
        time_range, start_datetime = parse_time_range_from_filename(filename)
        
        if time_range and start_datetime:
            time_groups[time_range].append(file_path)
            if time_range not in time_order:
                time_order[time_range] = start_datetime
        # ç§»é™¤unknown_timeå¤„ç†ï¼Œæ— æ³•è§£ææ—¶é—´çš„æ–‡ä»¶ç›´æ¥è·³è¿‡

    # æŒ‰æ—¶é—´æ’åº
    sorted_time_ranges = sorted(time_order.keys(), key=lambda x: time_order[x])
    
    # è¿”å›æœ‰åºå­—å…¸
    ordered_groups = {}
    for time_range in sorted_time_ranges:
        ordered_groups[time_range] = time_groups[time_range]
    
    return ordered_groups


def process_files_by_time_batches(output_base_dir, api_url, output_dir):
    """
    æŒ‰æ—¶é—´èŒƒå›´æ‰¹é‡å¤„ç†æ—¥å¿—æ–‡ä»¶
    
    Args:
        output_base_dir: é¢„å¤„ç†åçš„æ—¥å¿—æ–‡ä»¶ç›®å½•
        api_url: API URL
        output_dir: è¾“å‡ºç›®å½•
    """
    from tqdm import tqdm
    
    print("\n" + "="*60)
    print("ğŸ” å¼€å§‹æŒ‰æ—¶é—´æ‰¹é‡åˆ†ææ—¥å¿—æ–‡ä»¶")
    print("="*60)
    
    # è·å–æ‰€æœ‰åˆ†ç»„åçš„æ—¥å¿—æ–‡ä»¶
    file_list = UpLoad_File(output_base_dir)
    log_files = [f for f in file_list if f.endswith('.log')]
    
    if not log_files:
        print("âŒ æœªæ‰¾åˆ°åˆ†ç»„åçš„æ—¥å¿—æ–‡ä»¶")
        return
    
    print(f"ğŸ“ å‘ç° {len(log_files)} ä¸ªåˆ†ç»„æ—¥å¿—æ–‡ä»¶")
    
    # æŒ‰æ—¶é—´èŒƒå›´åˆ†ç»„
    time_groups = group_files_by_time_range(log_files)
    
    print(f"ğŸ“Š å…±åˆ†ä¸º {len(time_groups)} ä¸ªæ—¶é—´æ‰¹æ¬¡")
    
    # æŒ‰æ—¶é—´é¡ºåºå¤„ç†æ¯ä¸ªæ‰¹æ¬¡
    batch_num = 1
    total_batches = len(time_groups)
    
    for time_range, files in time_groups.items():
        print(f"\nğŸ•’ å¤„ç†æ—¶é—´æ‰¹æ¬¡ {batch_num}/{total_batches}: {time_range}")
        print(f"   åŒ…å« {len(files)} ä¸ªæ–‡ä»¶")
        
        # æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æ–‡ä»¶åˆ—è¡¨
        for file_path in files:
            filename = os.path.basename(file_path)
            print(f"   ğŸ“ {filename}")
        
        # å¯¹å½“å‰æ—¶é—´æ‰¹æ¬¡çš„æ‰€æœ‰æ–‡ä»¶è¿›è¡Œå¤„ç†
        print(f"   ğŸš€ å¼€å§‹åˆ†ææ—¶é—´æ‰¹æ¬¡ {time_range} çš„æ–‡ä»¶...")
        All_analysis = []
        for file_path in tqdm(files, desc=f"æ‰¹æ¬¡{batch_num}å¤„ç†è¿›åº¦", leave=False):
            try:
                output_dir = f'./Find_detect/output_529/{time_range}'
                all_analysis = File_D(file_path, api_url, output_dir)
                All_analysis.append(all_analysis)
                print(f"   âœ… å®Œæˆ: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥ {os.path.basename(file_path)}: {e}")
        
        print(f"   ğŸ‰ æ—¶é—´æ‰¹æ¬¡ {time_range} å¤„ç†å®Œæˆï¼")
        batch_num += 1

        # è°ƒç”¨å¤§æ¨¡å‹æ±‡æ€»åˆ†æç»“æœ
        print(f"   ğŸš€ å¼€å§‹æ±‡æ€»åˆ†ææ—¶é—´æ‰¹æ¬¡ {time_range} çš„æ–‡ä»¶...")
        dir = f'./Find_detect/output_529/{time_range}'
        final_result = analyze_log_directory(dir)                                                                   #     è°ƒç”¨æœ€åçš„å¤æ‚æ¨¡å‹å®Œæ•´æ ¹å› åˆ†æ
        # ä¿å­˜æ—¥å¿—åˆ†æç»“æœ
        analysis_result_dir = './Find_detect/output_529/analysis_result'
        os.makedirs(analysis_result_dir, exist_ok=True)
        
        # åˆ›å»ºåŒ…å«æ—¶é—´èŒƒå›´çš„æ–‡ä»¶å
        safe_time_range = time_range.replace(':', '-').replace(' ', '_')
        result_filename = f"analysis_{safe_time_range}.txt"
        result_filepath = os.path.join(analysis_result_dir, result_filename)
        
        # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
        try:
            with open(result_filepath, 'w', encoding='utf-8') as f:
                f.write(f"æ—¶é—´æ‰¹æ¬¡: {time_range}\n")
                f.write("="*60 + "\n")
                f.write(final_result)
            print(f"   ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result_filepath}")
        except Exception as e:
            print(f"   âŒ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
        break   # todo: debug
        # print(final_result)
    
    print(f"\nğŸ¯ æ‰€æœ‰æ—¶é—´æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“‚ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_dir}")


# File_D(r'C:\Users\pc\Desktop\code\log\logcot\log\log\log_controller_0_Event.txt','https://api.siliconflow.cn/v1/chat/completions',r'C:/Users/pc/Desktop/code/log/logcot/Find_detect/output_529')
def All_file(root_path, output_base_dir='./Find_detect/preprocessed_logs', group_hours=2):
    """
    å¤„ç†root_pathä¸­çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ï¼ŒæŒ‰æ—¶é—´åˆ†ç»„ä¿å­˜
    
    Args:
        root_path: è¾“å…¥æ—¥å¿—æ–‡ä»¶ç›®å½•
        output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
        group_hours: æ—¶é—´åˆ†ç»„é—´éš”ï¼ˆå°æ—¶ï¼‰
    
    Returns:
        dict: å¤„ç†ç»“æœç»Ÿè®¡
    """
    print("ğŸš€ å¼€å§‹æ—¥å¿—æ—¶é—´åˆ†ç»„å¤„ç†...")

    # debug å¦‚æœé‡Œé¢æœ‰output_base_diré‡Œlogæ–‡ä»¶ï¼Œå°±è·³è¿‡ä¸‹é¢çš„æ—¶é—´åˆ†ç»„åŠŸèƒ½
    if os.path.exists(output_base_dir): 
        processing_stats = {}
    else:
        # åˆ›å»ºæ—¶é—´åˆ†ç»„å™¨
        grouper = LogTimeGrouper(group_hours=group_hours)
        
        # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
        file_list = UpLoad_File(root_path)
        log_files = [f for f in file_list if grouper._is_log_file(os.path.basename(f))]
        
        print(f"ğŸ“ å‘ç° {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­ {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_base_dir, exist_ok=True)

        # æ¸…ç©ºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        if os.path.exists(output_base_dir):
            print(f"ğŸ§¹ æ¸…ç©ºè¾“å‡ºç›®å½•: {output_base_dir}")
            for filename in os.listdir(output_base_dir):
                file_path = os.path.join(output_base_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.remove(file_path) 
                    elif os.path.isdir(file_path):
                        import shutil
                        shutil.rmtree(file_path) 
                except Exception as e:
                    print(f"  âš ï¸ åˆ é™¤å¤±è´¥ {filename}: {e}")
            print("âœ… è¾“å‡ºç›®å½•æ¸…ç©ºå®Œæˆ")
        
        # å¤„ç†ç»Ÿè®¡
        processing_stats = {
            'total_files': len(log_files),
            'processed_files': 0,
            'total_groups': 0,
            'total_logs': 0,
            'failed_files': [],
            'grouped_files': []
        }
        
        # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºå¤„ç†è¿›åº¦
        from tqdm import tqdm
        
        for file_path in tqdm(log_files, desc="ğŸ“Š æ—¶é—´åˆ†ç»„è¿›åº¦"):
            try:
                print(f"\nğŸ” æ­£åœ¨å¤„ç†: {os.path.basename(file_path)}")
                
                # æŒ‰æ—¶é—´åˆ†ç»„å¤„ç†æ—¥å¿—æ–‡ä»¶ï¼Œä¼ é€’åŸºç¡€ç›®å½•ç”¨äºç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                grouped_logs = grouper.process_log_file(file_path, output_base_dir, base_dir=root_path)
                
                if grouped_logs:
                    # ç»Ÿè®¡ä¿¡æ¯
                    file_groups = len(grouped_logs)
                    file_logs = sum(len(logs) for logs in grouped_logs.values())
                    
                    processing_stats['processed_files'] += 1
                    processing_stats['total_groups'] += file_groups
                    processing_stats['total_logs'] += file_logs
                    
                    # è®°å½•ç”Ÿæˆçš„åˆ†ç»„æ–‡ä»¶
                    # ç”Ÿæˆä¸_save_grouped_logsä¸€è‡´çš„å”¯ä¸€æ–‡ä»¶åå‰ç¼€
                    try:
                        rel_path = os.path.relpath(file_path, root_path)
                        # å°†è·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼Œç”Ÿæˆå”¯ä¸€å‰ç¼€
                        unique_prefix = rel_path.replace(os.sep, '_').replace('/', '_').replace('\\', '_')
                        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
                        unique_prefix = os.path.splitext(unique_prefix)[0]
                    except ValueError:
                        # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨æ–‡ä»¶å
                        unique_prefix = os.path.splitext(os.path.basename(file_path))[0]
                    
                    for group_key in grouped_logs.keys():
                        if grouped_logs[group_key]:
                            grouped_file_path = os.path.join(output_base_dir, f"{unique_prefix}_{group_key}.log")
                            if os.path.exists(grouped_file_path):
                                processing_stats['grouped_files'].append({
                                    'file': grouped_file_path,
                                    'group': group_key,
                                    'log_count': len(grouped_logs[group_key])
                                })
                    
                    print(f"  âœ… å®Œæˆåˆ†ç»„: {file_groups} ä¸ªæ—¶é—´ç»„ï¼Œ{file_logs} æ¡æ—¥å¿—")
                    
                    # ç”Ÿæˆè¯¥æ–‡ä»¶çš„æ—¶é—´åˆ†ææŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
                    # report = grouper.generate_time_analysis_report(grouped_logs)
                    # report_path = os.path.join(output_base_dir, f"{base_name}_time_report.txt")
                    # with open(report_path, 'w', encoding='utf-8') as f:
                    #     f.write(report)
                        
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆæ—¶é—´ä¿¡æ¯")
                    
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                processing_stats['failed_files'].append({
                    'file': file_path,
                    'error': str(e)
                })
                continue
        
        # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ¯ å¤„ç†ç»“æœç»Ÿè®¡")
        print("="*60)
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {root_path}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_base_dir}")
        print(f"â° æ—¶é—´åˆ†ç»„é—´éš”: {group_hours} å°æ—¶")
        print(f"ğŸ“„ æ€»æ–‡ä»¶æ•°: {processing_stats['total_files']}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {processing_stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(processing_stats['failed_files'])}")
        print(f"ğŸ“Š ç”Ÿæˆæ—¶é—´ç»„: {processing_stats['total_groups']}")
        print(f"ğŸ“ æ€»æ—¥å¿—æ¡æ•°: {processing_stats['total_logs']}")
        print(f"ğŸ“ ç”Ÿæˆåˆ†ç»„æ–‡ä»¶: {len(processing_stats['grouped_files'])}")
        
        # æ˜¾ç¤ºç”Ÿæˆçš„åˆ†ç»„æ–‡ä»¶åˆ—è¡¨
        if processing_stats['grouped_files']:
            print(f"\nğŸ“‹ ç”Ÿæˆçš„åˆ†ç»„æ–‡ä»¶åˆ—è¡¨:")
            for item in processing_stats['grouped_files']:
                filename = os.path.basename(item['file'])
                print(f"  ğŸ“ {filename}: {item['log_count']} æ¡æ—¥å¿—")
        
        # æ˜¾ç¤ºå¤±è´¥çš„æ–‡ä»¶
        if processing_stats['failed_files']:
            print(f"\nâš ï¸ å¤„ç†å¤±è´¥çš„æ–‡ä»¶:")
            for item in processing_stats['failed_files']:
                filename = os.path.basename(item['file'])
                print(f"  âŒ {filename}: {item['error']}")
        
        print(f"\nğŸ‰ æ—¥å¿—æ—¶é—´åˆ†ç»„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“‚ æ‰€æœ‰åˆ†ç»„æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_base_dir}")

    # å¯¹é¢„å¤„ç†åçš„æ—¥å¿—è¿›è¡ŒæŒ‰æ—¶é—´æ‰¹é‡åˆ†æ
    # æ¸…ç©ºè¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    output_dir = r'./Find_detect/output_529'
    if os.path.exists(output_dir):
        print(f"ğŸ§¹ æ¸…ç©ºè¾“å‡ºç›®å½•: {output_dir}")
        for filename in os.listdir(output_dir):
            file_path = os.path.join(output_dir, filename)
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path) 
                elif os.path.isdir(file_path):
                    import shutil
                    shutil.rmtree(file_path) 
            except Exception as e:
                print(f"  âš ï¸ åˆ é™¤å¤±è´¥ {filename}: {e}")
    else:
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")

    start_time = time.time()
    
    process_files_by_time_batches(
        output_base_dir=output_base_dir,
        api_url='https://api.siliconflow.cn/v1/chat/completions',
        output_dir=output_dir
    )
    
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"\nâ±ï¸ ç¨‹åºè¿è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
    print(f"â±ï¸ ç¨‹åºè¿è¡Œæ—¶é—´: {execution_time/60:.2f} åˆ†é’Ÿ")
    
    return processing_stats


if __name__ == "__main__":
    # æ•°æ®é¢„å¤„ç†
    All_file(r'.\log\log')
